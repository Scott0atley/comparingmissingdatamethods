{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8455147e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T14:40:20.719188Z",
     "start_time": "2025-03-04T14:40:20.716796Z"
    }
   },
   "source": [
    "# A Simulation Study Comparing Handling Missing Data Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9ab13a",
   "metadata": {},
   "source": [
    "Scott Oatley (s2265605@ed.ac.uk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db020e76",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Missing data is a threat to the accurate reporting of substantive results within data analysis. While handling missing data strategies are widely available, many studies fail to account for missingness in their analysis. Those who do engage in handling missing data analysis sometimes engage in less than-gold-standard approaches that leave their substantive interpretation open toward methodological and statistical critique. These gold standard measures; multiple imputation (MI) and full information maximum likelihood (FIML), are rarely compared with one another. Mostly due to discipline-based preference, there has been little work on assessing if using one of these methods or the other would produce less biased estimates. This paper seeks to establish the real impact of implementing handling missing data methods upon statistical analysis and secondly, provide a direct comparison between multiple imputation and full information maximum likelihood methods of handling missing data. A simulation is performed on a variety of handling missing data strategies using a set seed for reproducibility. This simulation of methods is repeated 1000 times, and the 95% confidence intervals of the betas and standard errors of each model are presented to demonstrate the standard each handling missing data method provides the researcher. The results of this analysis confirm that under a missing at-random assumption, methods such as listwise deletion are not at all appropriate in handling missing data. Furthermore, naive methods are analysed, such as single-use imputation and found to be equally unattractive. Finally, results show that multiple imputations appear to provide the most accurate reporting of original ‘non-missing’ models compared to full information maximum likelihood techniques, though the difference is not large enough to rule it out as a viable ‘gold standard’. A discussion of statistical and time-based efficiency is also provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3610178e",
   "metadata": {},
   "source": [
    "### Key Words\n",
    "\n",
    "MCAR, MAR, MNAR, Multiple Imputation, FIML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080d3751",
   "metadata": {},
   "source": [
    "## Justification For Notebook\n",
    "\n",
    "Providing a literate workflow of any scientific investigations should be a baseline requiste of empirical research. Publishing a Jupyter Notebook allows individuals to not only reproduce the empirical work held within, it also provides a robust justification for each step in the research process. This process promotes open-science practices and encourages others to do the same. \n",
    "\n",
    "Encouraging others to openly look at ones work invites critique. These critiques are welcome. The hope of using Notebooks is to have a much more organic and engaging research process whereby the research does not simply end once a paper is published. Critical comments can and will be incorporated into this Notebook to further research practices. \n",
    "\n",
    "By providing a literate workflow where research, theory, justification, and 'footnote' analysis are all recorded in one place. This notebook invites a widespread auideance, ranging from other academics, to interested stakeholders. Whilst the language used in this Notebook is one intended for an academic auidence, the workflow presented should be possible for anyone that reads the Notebook and takes a methodical step-by-step approach to its application. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb50cb7",
   "metadata": {},
   "source": [
    "### Using Stata\n",
    "\n",
    "As Jupyter is a language agnostic program, the use of language used for analysis is left up to the individual researcher. For this Notebook, Stata is employed for all statistical analysis. Stata is a proprietary software and researchers MUST have access to Stata in order to undertake data analyses within the Jupyter notebook. A primary goal of extending this analysis is to undertake it in a different non-properiatory software. \n",
    "\n",
    "Using Stata requires enabling the use of the 'stata kernel' in Jupyter. The instructions for which have been outlined in meticulous details in Connelly and Gayle (2019). For the sake of promoting a consistent introduction base for Jupyter Notebooks, and in an attempt to avoid needless confusing rhetoric at the beginning of such Notebooks, their original instructions are pasted below. I thank them for their work in pioneering best practices in the use of Notebooks for social scientific analysis: \n",
    "\n",
    "#### Using Stata via Magic Cells\n",
    "\n",
    "The approach for this Notebook uses Stata via magic cells. This facility can be downloaded and installed from [this github repository](https://github.com/TiesdeKok/ipystata).\n",
    "\n",
    "At the command prompt you need to type:\n",
    "\n",
    "pip install ipystata\n",
    "\n",
    "In a code cell before using Stata you must type:\n",
    "\n",
    "import ipystata\n",
    "\n",
    "and then run the cell.\n",
    "\n",
    "Each cell will now be a Stata code cell as long as you start your syntax with:\n",
    "\n",
    "%%stata\n",
    "\n",
    "For example to get a summary of the variables in Stata the cell should include the following code:\n",
    "\n",
    "%%stata\n",
    "summarize\n",
    "\n",
    "further information on using Stata via magic [is available here](https://dev-ii-seminar.readthedocs.io/en/latest/notebooks/Stata_in_jupyter.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c1add",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "- [Background](#Background)\n",
    "- [Missing Data Theory](#Missing)\n",
    "- Simulated Data\n",
    "- Preparation Programs\n",
    "- Descriptive Statistics \n",
    "- Missingness Models\n",
    "- Discussion of Results\n",
    "- Conclusions\n",
    "- Notes\n",
    "- Supplementary Materials\n",
    "- References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a042887e",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Missing Data pervades social scientific research. Without appropriately accounting for missing data in reserach agendas, this missingness can have stark consequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9db1c3",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Missing data is a term that describes an instance whereby either an item (variable) or unit (person/entire observation) is missing from the sample. Missing data is common within social science research and even more ubiquitous within a longitudinal context where wave attrition and non-response are a rule rather than exception. The primary concern surrounding missing data is that dependent upon the type of missingness, there is potential to affect inferences made by the analysis of longitudinal studies (Hawkes and Plewis, 2006: 479; Silverwood et al., 2021). The various factors that account for sample attrition in the datasets presented thus far have the potential to present real issues as they relate to comprehensive data analysis. \n",
    "\n",
    "Missingness attributed to death or emigration are considered ‘natural’ from the original sample. Those, however, that either refuse continued survey participation or complete surveys partially may present problems of biased estimates if the missingness is not appropriately discussed. Put simply, if there is an identified pattern within the missing data present within a given analysis this has the potential to influence results and alter substantive conclusions had this missingness not occurred – or been appropriately accounted for. \n",
    "\n",
    "When a unit (case/observation) provides information on every possible item (variable) within a survey, this is a complete record. When information on certain items within a survey are missing, this is then called an incomplete record. When only part of all possible items is missing from the unit this is called item non-response. When all possible items are missing this is called unit non-response. Unit non-response is a natural consequence of a longitudinal survey design. The statistical techniques discussed henceforth will focus on item non-response because it is far more common in practice than the alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60edc182",
   "metadata": {},
   "source": [
    "# Missing Data Theory\n",
    "\n",
    "The investigation of missingness within data, through the creation of tables and graphical summaries of the full data compared to partially observed variables can beginning to identify the mechanisms underlying the patterns of missingness. The terminology that defines these mechanisms is unfortunately rather obtuse and confusing. The follow paragraphs attempt to best explain and define the key missingness mechanisms that are used with the methodological literature surrounding missing data whilst attempting to provide a clear and concise understanding. \n",
    "\n",
    "The first major missingness mechanism is called Missing Completely At Random (MCAR).  Suppose that only one variable Y has missing data, and that another set of variables represented by the vector X, is always observed (Marsden and Wright, 2010). The data is MCAR if the probability that Y is missing does not depend on either X or Y itself. Evaluating the assumption that missingness on Y depends on some observed variable in X is straightforward. Allison (2012) uses the example of income depending on gender by testing whether the proportions of men and women who report their income differ – a logistic regression in which the dependent variable is the response indicator could be estimated, and significant coefficients would suggest a violation of the MCAR mechanism (ibid). Testing whether missingness on Y does not depend on Y itself is much more complicated. Unless we have existing linked data such as tax records in the income example, it is almost impossible to evaluate this assumption. The upside of an MCAR mechanism is that estimated coefficients will not provide biased results in the presence of data Missing Completely At Random, however their estimation may be less precise (Kang, 2013). \n",
    "\n",
    "The second missingness mechanism is missing at random (MAR). Data on Y is considered MAR if the probability that Y is missing does not depend on Y, once we control for X. MAR allows for missingness on Y to depend on other variables so long as it does not depend on Y itself. \n",
    "\n",
    "Finally, missing not at random (MNAR) means missingness depends on unobserved values (Silverwood et al. 2021), and that the probability that Y is missing depends on Y itself, after adjusting for X (Marsden and Wright, 2010). For example, people who have been arrested may be less likely to report their arrest status. \n",
    "\n",
    "If data is found to be MAR or MCAR, then approaches like multiple imputation (MI), Full Information Maximum likelihood (FIML), and inverse probability weighting (IPW) are made available – the former being extensively documented with the NCDS (Hawkes and Plewis 2006). These ‘gold standard’ approaches to handling missing data have also been found to produce optimal estimates in the MNAR case but it is difficult to have confidence that any given MNAR model is correct (Marsden and Wright, 2010)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac69356",
   "metadata": {},
   "source": [
    "# Methods to Handle Missing Data\n",
    "\n",
    "The following section explores each major attempt at handling missing data. These techniques and methods are bundled into ‘standards’ to express the level of appropriate application most of these approaches fall into. While the majority are placed in ‘less than gold standard’ approaches, that does not necessarily make them devoid of utility. There are three main categories that handling missing data techniques fall into: ‘gold standard’, ‘less than gold standard’, and ‘inadequate standard’. Each method for each category will be explained. \n",
    "\n",
    "## Inadequate Standard\n",
    "\n",
    "The first inadequate standard given a MAR mechanism is present is listwise deletion. Listwise deletion removes all observations from the data with a missing value in one or more of the variables included in the analysis. This is also known as Complete Records Analysis (CRA). The CRA approach is unpredictable; there is no way to know the consequences of this loss of information if data is found to be MAR (Carpenter and Kenward, 2012). When data is found to be MAR, a CRA approach is inadequate at handling missing data.\n",
    "\n",
    "Depending on the variable (either metric or categorical) a simple approach to handling missing data would be to use a single mean or single modal imputation. This, in the example of a categorical variable, takes the mode of the value in said variable and imputes that modal value across all missing values in the data. Single imputation ignores all uncertainty and always underestimates the variance in each model. Advocates of this approach argue that whilst not perfect this approach doesn’t delete a single case and incorporates all available information into a given model. However, this method does not have any confidence in its results. There is a possibility that the estimates from this method may fall close to the true range; of course, the exact opposite is equally likely. The use of single-use imputation has been consistently and conclusively shown to perform poorly except under exceptionally special conditions (Collins, Schafer and Kam, 2001; Little and Rubin, 2019). For these reasons, single use imputation is an inadequate method to handle missing data. \n",
    "\n",
    "## Less than Gold Standard \n",
    "\n",
    "Dummy variable adjustment is another method of handling missing data that falls into the ‘less than gold standard’ category. Dummy variable adjustment may appear to be in the same category of handling missing data methods as single use imputation. This is, however, not the case. Dummy variable adjustment is where all missingness at the given variable is coded to a value within the model. In the example of a binary dummy variable, all missingness is coded to either equal zero or equal one. This does have the identical appeal to the single-use imputation of deleting no cases and incorporates all information into the regression model. However, there is a substantive difference between the two techniques. For the simple model of data missing at Y variable, a dummy variable adjustment will not provide the ‘true’ estimates but if the complete records analysis is compared to a model where all missingness equals zero and another model where all missingness equals one, then the range of the estimates can be located. Whilst Jones (1996) demonstrated that dummy variable adjustment yields biased parameter estimates even when the data is MCAR, the ability to provide a range of the estimates does provide some utility to this technique. Given a MAR example where the reported estimates are a reduced form from their ‘true’ values, iff the complete case analysis and both dummy variable adjustment models present a beta coefficient that is throughout all models positive, one can present those results similar to how we ought to interpret log odds. The results would present evidence for a positive coefficient – though the exact size is unknown, some information can be gathered. For this reason, dummy variable adjustment provides some utility in certain missing data scenarios. This technique has the most utility in scenarios where missingness is so great that it begins to stretch the abilities of even gold-standard techniques. This method for handling missing data is not perfect, but it does provide utility and allows the use of data that has large amounts of missingness. \n",
    "\n",
    "Another method that deals with missing data is the use of survey weights. Survey weights consider missingness. Inverse Probability Weighting (IPW) creates weighted copies of complete records to remove selection bias introduced by missing data. Whilst IPW is a method of dealing with missing data, alternatives such as multiple imputation are regarded as much more efficient as IPW only determines weights from incomplete cases and partially observed cases are discarded in the weighted analysis. Due to this, weighted estimates can have unacceptably high variance (Seaman et al., 2012; Seaman and White, 2013; Little, Carpenter and Lee, 2022).As such IPWs are placed in the ‘less than gold standard’ category of handling missing data approaches.  \n",
    "\n",
    "## Gold Standard\n",
    "\n",
    "There are two ‘gold standard’ approaches to handling missing data, Multiple Imputation (MI) and Maximum Likelihood (ML). Referring to the latter method first, there are currently three ML estimation algorithms for use when missing data is present with either an MCAR or MAR mechanism. The first is the multiple-group method, whereby a sample is divided into subgroups which each share the same pattern of missing data. A likelihood function is computed for each of the subgroups and the groupwise likelihood functions are accumulated across the entire sample and maximised. There are some practical issues of implementing this multiple-group based ML approach (Enders, 2001). The major drawback of this approach however is that it is a group level, rather than individual level ML estimation. Another ML estimation is the expectation-maximisation (EM). This estimation uses a two-step iterative procedure where missing observations are filled in or imputed and the unknown parameters are estimated using maximum likelihood missing data algorithms. The EM approach can only be used to obtain ML estimates of a mean vector and covariance matrix and as a result standard errors will be negatively biased and bootstrapping is recommended (Enders, 2001). The final ML approach discussed here is the Full Information Maximum Likelihood (FIML) estimation. It has also been called the raw maximum likelihood estimation for its likelihood function being calculated at the individual. It is also exceptionally easy to implement compared to the other estimation procedures discussed (Enders, 2001). For these reasons, going forward ML discussions of handling missing data specifically refer to the FIML approach rather than the multiple-group or EM approach. \n",
    "\n",
    "Multiple Imputation is the second of the ‘Gold standard’ handling missing data methods. Multiple imputation generates replacement values or imputations for the missing data values and repeats this procedure over many iterations to produce a ‘semi-Bayesian’ framework for the most appropriate fit of estimates. For multiple imputation models to be compared to a complete records analysis, the former needs to be ‘‘congenial’’ (White, Royston and Wood, 2011) with the latter. Congeniality or consistency in this respect means that the same variables in the complete record analysis are identical to those included in multiple imputation. Suppose the variables between complete records analysis and multiple imputation models differ. In that case, the correct variance/covariance matrix will not be estimated, and a substantive comparison between the two will become impossible and impracticable due to a loss of statistical power (Von Hippel, 2009; Lynch and Von Hippel, 2013). \n",
    "\n",
    "Multivariate imputation by chained equations (MICE) is a form of multiple imputation that fills in or imputes missing data within a given dataset through iterative predictive models or k imputations. This specification is required when imputing a variable that must only take on specific values, such as the categorical nature of the economic activity response variable within the current analytical model. Using MICE, each imputation k is drawn from the posterior distribution of the parameters in the given imputation model, and then the model itself is imputed (Carpenter and Kenward, 2012). To create the kth imputation, new parameters are drawn from the posterior distribution. Multiple Imputation following MICE draws from Bayesian influences on the distribution of missing data upon observed data. An essential advantage of Multiple Imputation is that it can be applied for data missing at the response variable or its covariates (Carpenter and Kenward, 2012).\n",
    "\n",
    "Multiple imputation uses auxiliary variables – variables not included in the main model but are used when setting the data to be imputed. The auxiliary variables' main function is to improve the predictive ability of the imputation model over and above the information recovered from just using the information provided by the analytical variables in the model (Collins, Schafer and Kam, 2001). Auxiliary variables are essential when there are high levels of missingness upon a given variable (Johnson and Young, 2011; Young and Johnson, 2011). There is no strict threshold for what an auxiliary variable needs to be included within the imputation; however, some have recommended an r > 0.4 on at least one of the analytical variables within the model (Allison, 2012a). However, this is disputed (Enders, 2010). Others, such as Silverwood et al. (2021), argue that if an auxiliary variable is predictive of the outcome variable, it makes them suitable for inclusion within the imputation model. An auxiliary variable does not have the requirement that the given variable has to have complete information to be valuable – auxiliary variables can still be influential when they have missingness (Enders, 2010). \n",
    "\n",
    "Multiple Imputation can be implemented easily and readily across software platforms, unlike FIML. Multiple imputation does, however, have some drawbacks. It can be a lengthy procedure that has the potential to induce human error due to the need to select auxiliary variables, set the correct data for imputation, and set the correct seed etc. There is also a time efficiency argument, whereby for multiple imputation, if the dataset is large, or there are large amounts of missingness, then the time to impute the model of interest can take a large amount of time. MI is an attractive method because it is practical and widely applicable (Carpenter and Kenward, 2012). \n",
    "\n",
    "Whilst original literature on missing data and MI typically referred to large datasets with marginal levels of missingness, contemporary studies and simulations have increasingly stretched and stress-tested the limits of MI (Hardt et al., 2013). A simulation by Hardt et al (2013) demonstrated that large amounts of missingness can be present within a model without breaking down MI or FIML mechanisms (ibid). Whilst their simulation stops at n=200 where 40 per cent missingness is acceptable, there general argument is that the greater the n the larger the missingness can be within a model without breaking MI or FIML so long as the models themselves are appropriately specified. Imputation-based models are consistently found to outperform a CRA in both absolute bias and Root Mean Squared Error (RMSE) with increasing levels of missingness (Hyuk Lee and Huber Jr., 2021). The most extreme case from Madely-Down et al (2019) demonstrates that so long as the imputation model is properly specified and data are MAR then unbiased results can be obtained even with up to 90 per cent missingness. An imputation model compared to a CRA can achieve a reduction in 99.97 per cent bias when missingness is at 90 per cent (ibid).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0962574f",
   "metadata": {},
   "source": [
    "# Comparisons of Gold Standard Methods\n",
    "\n",
    "Paul Allison, in a series of articles (Allison, 2012a, 2012b, 2015), argues that FIML is 1) more straightforward to implement, 2) FIML has no incompatibility between an imputation model and an analysis model, 3) FIML produces a deterministic result rather than a different result every time, and 4) FIML is asymptomatically efficient. \n",
    "\n",
    "Firstly, MI does have greater variability than FIML, but that increased choice in model selection is not necessarily a negative so long as proper procedures are followed. In fact, greater variability of choice has the potential to make MI a more attractive candidate for dealing with missingness over FIML. Secondly, MI models only run into an incompatibility problem when the MI model is inconsistent with the CRA model – something that, with appropriate testing and open science practices detailing the model construction, should not happen. Thirdly, MI models are deterministic, provided the same seed is used each time you run the imputation. The only time this would not be plausible would be when open science practices were not followed, and fellow researchers could not access the MI seed. Finally, the argument that FIML is asymptotically efficient only holds to a certain extent. MI models reach asymptotic efficiency by running an infinite number of imputations – though you can reach near full efficiency with a relatively small number of imputations, Allison (2015) argues, around 10. Overall, whilst FIML does offer some advantages, there is nothing so considerable theoretically as to desire FIML over MI on the condition that they both perform at near identical rates. So long as open science procedures are upheld, most major critiques of MI are dealt with. \n",
    "\n",
    "There are very few comparisons between FIML and MI approaches to missing data, making it hard to assess whether one method is more efficient at dealing with missing data than the other. Before conducting any missing data methods on the NCDS data, a simulation is performed to assess the strengths of a range of handling missing data approaches, with the intent to directly compare FIML and MI methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef183245",
   "metadata": {},
   "source": [
    "# Methods\n",
    "\n",
    "Both FIML and MI practices require data to either be MCAR or MAR. A FIML approach can be achieved in Stata by using the ‘sem’ command – using structural equation modelling and using the ‘mlvm’ estimation option (mlvm means FIML). MI can also be achieved in Stata using the ‘mi’ commands using a semi-Bayesian approach that includes auxiliary variables. There are also other handling missing data methods available such as: single mean imputation and coding all data=0 OR =1. These practices are typically considered ‘bad’ ways of handling missing data but are included in the simulation as a comparison to FIML and MI methods. \n",
    "\n",
    "The full simulation  takes the form of 1000 iterations of a random normal distribution of 1000 observations around a normally distributed metric dependent variable and three independent dummy variables that share an identical distribution. Each independent variable has the same level of correlation associated with the dependent variable. This is to allow for a point of comparison when MAR missingness is injected into one variable and not the others to see what happens when handling missing data practices are implemented. Each model is isolated in its own program whereby a simulation is called using the programs function with an identical seed set to all models. The 95 per cent confidence intervals of the mean betas and standard errors for all variables within each model are gathered and reported. \n",
    "\n",
    "This dependent variable and three independent variables form a basic OLS linear regression model that is called the ‘Complete Records God Model’. Named as such because no model in a normal social scientific framework would have all observations not missing and have prior knowledge of what the ‘complete’ model would have looked like if their model did have some element of missingness. In addition to this ‘God’ model the same regression is computed using the structural equation modelling framework in Stata to confirm the results would be identical. The next model is where missingness is introduced. Missingness is injected into independent variable three. This missingness accounts for 49 per cent missingness in the model. This amount of missingness is right on the cusp of what contemporary literature on multiple imputation and FIML allow. Dummy variable adjustment is produced whereby all missingness is coded as =0 and another is produced =1. Next a single use modal imputation is used – the same framework as a single use mean imputation but because the variable is categorical mode is used over mean. Finally, an FIML model under the SEM framework is produced alongside three different forms of Multiple Imputation models. The first is an MI with 10 imputations and no auxiliary variables, the second is an MI with 10 imputations and auxiliary variables, and finally the last model is an MI with 100 imputations and auxiliary variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e24150",
   "metadata": {},
   "source": [
    "# Preparation Programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7129909",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:25:16.141863Z",
     "start_time": "2025-04-05T10:25:16.138860Z"
    }
   },
   "outputs": [],
   "source": [
    "import stata_setup\n",
    "\n",
    "stata_setup.config(\"C:\\Program Files\\Stata18\", \"se\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d550484",
   "metadata": {},
   "source": [
    "Prior to any simulation/analysis, some basic set up is required to save datasets and path folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9146338e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:25:22.414949Z",
     "start_time": "2025-04-05T10:25:22.375114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". \n",
      ". global workingdata\"G:\\Stata data and do\\do files\\missing data\\workingdata\"\n",
      "\n",
      ". \n",
      ". cd \"$workingdata\"\n",
      "G:\\Stata data and do\\do files\\missing data\\workingdata\n",
      "\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "%%stata\n",
    "\n",
    "global workingdata\"G:\\Stata data and do\\do files\\missing data\\workingdata\"\n",
    "\n",
    "cd \"$workingdata\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ba1650",
   "metadata": {},
   "source": [
    "The first program, called \"regressbase\" simulates a dataset with 1000 observations. Four variables are generated to simulate a standard Ordinary Least Squares linear regression model with one y dependent variable and three x* independent variables. All variables, for ease of interpretation are generated as metric continuous variables. All variables are drawn from a normal distribution and their means and standard deviations are set at different amounts. This is to simulate the variety of independent variables that are typically used in a regression model. The dependent variable y is generated through a relationship to the independent variables to guarantee a significant relationship. The generation of y also sees the addition of an error term that is normally distributed. \n",
    "\n",
    "This simulated dataset forms the base of all subsequent programs. \n",
    "\n",
    "For the \"regressbase\" program, following the generation of variables, an OLS regression is performed and the betas and standard errors of each independent variable is stored as a newly generated variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "178d72cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:25:23.937992Z",
     "start_time": "2025-04-05T10:25:23.898962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". \n",
      ". capture program drop regress1\n",
      "\n",
      ". \n",
      ". program regress1, rclass\n",
      "  1. drop _all\n",
      "  2.     \n",
      ". * Set number of observations\n",
      ". set obs 1000\n",
      "  3.     \n",
      ". * Generate metric independent variable #1\n",
      ". drawnorm x1, n(1000) means(40) sds(12)\n",
      "  4. \n",
      ". * Generate metric independent variable #2\n",
      ". drawnorm x2, n(1000) means(200) sds(50)\n",
      "  5. \n",
      ". * Generate metric independent variable #3\n",
      ". drawnorm x3, n(1000) means(150) sds(5)\n",
      "  6. \n",
      ". * Generate metric dependent variable\n",
      ". gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n",
      "  7. \n",
      ". regress y x1 x2 x3\n",
      "  8. \n",
      ". gen beta1 = _b[x1]\n",
      "  9. gen se1 = _se[x1]\n",
      " 10. gen lower1 = beta1 - 1.96 * se1\n",
      " 11. gen upper1 = beta1 + 1.96 * se1\n",
      " 12. \n",
      ". gen beta2 = _b[x2]\n",
      " 13. gen se2 = _se[x2]\n",
      " 14. gen lower2 = beta2 - 1.96 * se2\n",
      " 15. gen upper2 = beta2 + 1.96 * se2\n",
      " 16. \n",
      ". gen beta3 = _b[x3]\n",
      " 17. gen se3 = _se[x3]\n",
      " 18. gen lower3 = beta3 - 1.96 * se3\n",
      " 19. gen upper3 = beta3 + 1.96 * se3\n",
      " 20. \n",
      ". \n",
      ". end\n",
      "\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "%%stata\n",
    "\n",
    "capture program drop regress1\n",
    "\n",
    "program regress1, rclass\n",
    "drop _all\n",
    "    \n",
    "* Set number of observations\n",
    "set obs 1000\n",
    "    \n",
    "* Generate metric independent variable #1\n",
    "drawnorm x1, n(1000) means(40) sds(12)\n",
    "\n",
    "* Generate metric independent variable #2\n",
    "drawnorm x2, n(1000) means(200) sds(50)\n",
    "\n",
    "* Generate metric independent variable #3\n",
    "drawnorm x3, n(1000) means(150) sds(5)\n",
    "\n",
    "* Generate metric dependent variable\n",
    "gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n",
    "\n",
    "regress y x1 x2 x3\n",
    "\n",
    "gen beta1 = _b[x1]\n",
    "gen se1 = _se[x1]\n",
    "gen lower1 = beta1 - 1.96 * se1\n",
    "gen upper1 = beta1 + 1.96 * se1\n",
    "\n",
    "gen beta2 = _b[x2]\n",
    "gen se2 = _se[x2]\n",
    "gen lower2 = beta2 - 1.96 * se2\n",
    "gen upper2 = beta2 + 1.96 * se2\n",
    "\n",
    "gen beta3 = _b[x3]\n",
    "gen se3 = _se[x3]\n",
    "gen lower3 = beta3 - 1.96 * se3\n",
    "gen upper3 = beta3 + 1.96 * se3\n",
    "\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b023ee50",
   "metadata": {},
   "source": [
    "The second program, \"sem1\" is nearly identical to the first program. The difference lies in the regression model. Whereas the \"regressbase\" program uses OLS linear regression, the \"sem1\" program uses structural equation modeling with maximum likelihood with missing values estimation. These 'base' models should provide identical statistics but both are provided to assess and potential disparities between the two models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa418980",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:25:25.002126Z",
     "start_time": "2025-04-05T10:25:24.961369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". \n",
      ". \n",
      ". capture program drop sem1\n",
      "\n",
      ". \n",
      ". program sem1, rclass\n",
      "  1.  drop _all\n",
      "  2.     \n",
      ". * Set number of observations\n",
      ". set obs 1000\n",
      "  3.     \n",
      ". * Generate metric independent variable #1\n",
      ". drawnorm x1, n(1000) means(40) sds(12)\n",
      "  4. \n",
      ". * Generate metric independent variable #2\n",
      ". drawnorm x2, n(1000) means(200) sds(50)\n",
      "  5. \n",
      ". * Generate metric independent variable #3\n",
      ". drawnorm x3, n(1000) means(150) sds(5)\n",
      "  6. \n",
      ". * Generate metric dependent variable\n",
      ". gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n",
      "  7. \n",
      ". sem (y <- x1 x2 x3), method(mlmv)\n",
      "  8. \n",
      ". gen beta1 = _b[x1]\n",
      "  9. gen se1 = _se[x1]\n",
      " 10. gen lower1 = beta1 - 1.96 * se1\n",
      " 11. gen upper1 = beta1 + 1.96 * se1\n",
      " 12. \n",
      ". gen beta2 = _b[x2]\n",
      " 13. gen se2 = _se[x2]\n",
      " 14. gen lower2 = beta2 - 1.96 * se2\n",
      " 15. gen upper2 = beta2 + 1.96 * se2\n",
      " 16. \n",
      ". gen beta3 = _b[x3]\n",
      " 17. gen se3 = _se[x3]\n",
      " 18. gen lower3 = beta3 - 1.96 * se3\n",
      " 19. gen upper3 = beta3 + 1.96 * se3\n",
      " 20. \n",
      ". end\n",
      "\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "%%stata\n",
    "\n",
    "\n",
    "capture program drop sem1\n",
    "\n",
    "program sem1, rclass\n",
    " drop _all\n",
    "    \n",
    "* Set number of observations\n",
    "set obs 1000\n",
    "    \n",
    "* Generate metric independent variable #1\n",
    "drawnorm x1, n(1000) means(40) sds(12)\n",
    "\n",
    "* Generate metric independent variable #2\n",
    "drawnorm x2, n(1000) means(200) sds(50)\n",
    "\n",
    "* Generate metric independent variable #3\n",
    "drawnorm x3, n(1000) means(150) sds(5)\n",
    "\n",
    "* Generate metric dependent variable\n",
    "gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n",
    "\n",
    "sem (y <- x1 x2 x3), method(mlmv)\n",
    "\n",
    "gen beta1 = _b[x1]\n",
    "gen se1 = _se[x1]\n",
    "gen lower1 = beta1 - 1.96 * se1\n",
    "gen upper1 = beta1 + 1.96 * se1\n",
    "\n",
    "gen beta2 = _b[x2]\n",
    "gen se2 = _se[x2]\n",
    "gen lower2 = beta2 - 1.96 * se2\n",
    "gen upper2 = beta2 + 1.96 * se2\n",
    "\n",
    "gen beta3 = _b[x3]\n",
    "gen se3 = _se[x3]\n",
    "gen lower3 = beta3 - 1.96 * se3\n",
    "gen upper3 = beta3 + 1.96 * se3\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f3e2d4",
   "metadata": {},
   "source": [
    "The third program, \"missingmcar\" generates a Missing Completely At Random (MCAR) mechanism and injects this into the OLS linear regression model. The MCAR mechanism is generated by first generating a new mcar varibale that is based on a binomial distribution where 50 per cent of values are coded as 1 and 50 per cent are coded as 0. This is then used to replace values in x2 equal to missing if values in mcar equal zero. This simulates a MCAR mechanism of missing completely at random. Under this mechanism, missingness even as much as 50 per cent should not alter the interpretation of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "211485dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:25:25.877478Z",
     "start_time": "2025-04-05T10:25:25.837466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". \n",
      ". capture program drop missingmcar\n",
      "\n",
      ". \n",
      ". program missingmcar, rclass\n",
      "  1.  drop _all\n",
      "  2.     \n",
      ". * Set number of observations\n",
      ". set obs 1000\n",
      "  3.     \n",
      ". * Generate metric independent variable #1\n",
      ". drawnorm x1, n(1000) means(40) sds(12)\n",
      "  4. \n",
      ". * Generate metric independent variable #2\n",
      ". drawnorm x2, n(1000) means(200) sds(50)\n",
      "  5. \n",
      ". * Generate metric independent variable #3\n",
      ". drawnorm x3, n(1000) means(150) sds(5)\n",
      "  6. \n",
      ". * Generate metric dependent variable\n",
      ". gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n",
      "  7. \n",
      ". * Generate MCAR \n",
      ". gen rmcar = rbinomial(1, 0.5)  // MCAR: 50% chance of missingness (binary ran\n",
      "> dom)\n",
      "  8. replace x2 = . if rmcar == 0  // Set x to missing where rmcar == 0\n",
      "  9. \n",
      ". regress y x1 x2 x3\n",
      " 10. \n",
      ". gen beta1 = _b[x1]\n",
      " 11. gen se1 = _se[x1]\n",
      " 12. gen lower1 = beta1 - 1.96 * se1\n",
      " 13. gen upper1 = beta1 + 1.96 * se1\n",
      " 14. \n",
      ". gen beta2 = _b[x2]\n",
      " 15. gen se2 = _se[x2]\n",
      " 16. gen lower2 = beta2 - 1.96 * se2\n",
      " 17. gen upper2 = beta2 + 1.96 * se2\n",
      " 18. \n",
      ". gen beta3 = _b[x3]\n",
      " 19. gen se3 = _se[x3]\n",
      " 20. gen lower3 = beta3 - 1.96 * se3\n",
      " 21. gen upper3 = beta3 + 1.96 * se3\n",
      " 22. \n",
      ". \n",
      ". end\n",
      "\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "%%stata \n",
    "\n",
    "capture program drop missingmcar\n",
    "\n",
    "program missingmcar, rclass\n",
    " drop _all\n",
    "    \n",
    "* Set number of observations\n",
    "set obs 1000\n",
    "    \n",
    "* Generate metric independent variable #1\n",
    "drawnorm x1, n(1000) means(40) sds(12)\n",
    "\n",
    "* Generate metric independent variable #2\n",
    "drawnorm x2, n(1000) means(200) sds(50)\n",
    "\n",
    "* Generate metric independent variable #3\n",
    "drawnorm x3, n(1000) means(150) sds(5)\n",
    "\n",
    "* Generate metric dependent variable\n",
    "gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n",
    "\n",
    "* Generate MCAR \n",
    "gen rmcar = rbinomial(1, 0.5)  // MCAR: 50% chance of missingness (binary random)\n",
    "replace x2 = . if rmcar == 0  // Set x to missing where rmcar == 0\n",
    "\n",
    "regress y x1 x2 x3\n",
    "\n",
    "gen beta1 = _b[x1]\n",
    "gen se1 = _se[x1]\n",
    "gen lower1 = beta1 - 1.96 * se1\n",
    "gen upper1 = beta1 + 1.96 * se1\n",
    "\n",
    "gen beta2 = _b[x2]\n",
    "gen se2 = _se[x2]\n",
    "gen lower2 = beta2 - 1.96 * se2\n",
    "gen upper2 = beta2 + 1.96 * se2\n",
    "\n",
    "gen beta3 = _b[x3]\n",
    "gen se3 = _se[x3]\n",
    "gen lower3 = beta3 - 1.96 * se3\n",
    "gen upper3 = beta3 + 1.96 * se3\n",
    "\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2ae5a7",
   "metadata": {},
   "source": [
    "The fourth program, \"missingmar\" follows a similar pattern to the \"missingmcar\" program but instead of injecting a mcar mechanism, it injects a Missing At Random (MAR) mechanism. The MAR mechanism is generated by first generating a new probability of MAR variable based on the logistic curve of y-2105.2. (The -2105.2 is used to center y in order to ascertain a 50 per cent missingness mechanism). The values of x2 are made equal to missing if the probability of MAR variable is equal to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9487b381",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:25:26.679756Z",
     "start_time": "2025-04-05T10:25:26.640743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". \n",
      ". capture program drop missingmar\n",
      "\n",
      ". \n",
      ". program missingmar, rclass\n",
      "  1.  drop _all\n",
      "  2.     \n",
      ". * Set number of observations\n",
      ". set obs 1000\n",
      "  3.     \n",
      ". * Generate metric independent variable #1\n",
      ". drawnorm x1, n(1000) means(40) sds(12)\n",
      "  4. \n",
      ". * Generate metric independent variable #2\n",
      ". drawnorm x2, n(1000) means(200) sds(50)\n",
      "  5. \n",
      ". * Generate metric independent variable #3\n",
      ". drawnorm x3, n(1000) means(150) sds(5)\n",
      "  6. \n",
      ". * Generate metric dependent variable\n",
      ". gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n",
      "  7. \n",
      ". * Generate MAR\n",
      ". gen prob_mar = logistic(y-21791)\n",
      "  8. gen rmar = 0 if prob_mar==0\n",
      "  9. replace x2 = . if rmar == 0  // Set x to missing where rmar == 0\n",
      " 10. regress y x1 x2 x3\n",
      " 11. \n",
      ". gen beta1 = _b[x1]\n",
      " 12. gen se1 = _se[x1]\n",
      " 13. gen lower1 = beta1 - 1.96 * se1\n",
      " 14. gen upper1 = beta1 + 1.96 * se1\n",
      " 15. \n",
      ". gen beta2 = _b[x2]\n",
      " 16. gen se2 = _se[x2]\n",
      " 17. gen lower2 = beta2 - 1.96 * se2\n",
      " 18. gen upper2 = beta2 + 1.96 * se2\n",
      " 19. \n",
      ". gen beta3 = _b[x3]\n",
      " 20. gen se3 = _se[x3]\n",
      " 21. gen lower3 = beta3 - 1.96 * se3\n",
      " 22. gen upper3 = beta3 + 1.96 * se3\n",
      " 23. \n",
      ". \n",
      ". end\n",
      "\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "%%stata\n",
    "\n",
    "capture program drop missingmar\n",
    "\n",
    "program missingmar, rclass\n",
    " drop _all\n",
    "    \n",
    "* Set number of observations\n",
    "set obs 1000\n",
    "    \n",
    "* Generate metric independent variable #1\n",
    "drawnorm x1, n(1000) means(40) sds(12)\n",
    "\n",
    "* Generate metric independent variable #2\n",
    "drawnorm x2, n(1000) means(200) sds(50)\n",
    "\n",
    "* Generate metric independent variable #3\n",
    "drawnorm x3, n(1000) means(150) sds(5)\n",
    "\n",
    "* Generate metric dependent variable\n",
    "gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n",
    "\n",
    "* Generate MAR\n",
    "gen prob_mar = logistic(y-21791)\n",
    "gen rmar = 0 if prob_mar==0\n",
    "replace x2 = . if rmar == 0  // Set x to missing where rmar == 0\n",
    "regress y x1 x2 x3\n",
    "\n",
    "gen beta1 = _b[x1]\n",
    "gen se1 = _se[x1]\n",
    "gen lower1 = beta1 - 1.96 * se1\n",
    "gen upper1 = beta1 + 1.96 * se1\n",
    "\n",
    "gen beta2 = _b[x2]\n",
    "gen se2 = _se[x2]\n",
    "gen lower2 = beta2 - 1.96 * se2\n",
    "gen upper2 = beta2 + 1.96 * se2\n",
    "\n",
    "gen beta3 = _b[x3]\n",
    "gen se3 = _se[x3]\n",
    "gen lower3 = beta3 - 1.96 * se3\n",
    "gen upper3 = beta3 + 1.96 * se3\n",
    "\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66fff2c",
   "metadata": {},
   "source": [
    "The program \"missingmean\" is the first handling missing data method implemented. This program performs single mean imputation across the 50 per cent missingness within the model. The mean of x2 is generated and used to replace all missing values in x2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22997fca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:25:27.504639Z",
     "start_time": "2025-04-05T10:25:27.465934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". \n",
      ". capture program drop missingmean\n",
      "\n",
      ". \n",
      ". program missingmean, rclass\n",
      "  1.  drop _all\n",
      "  2.     \n",
      ". * Set number of observations\n",
      ". set obs 1000\n",
      "  3.     \n",
      ". * Generate metric independent variable #1\n",
      ". drawnorm x1, n(1000) means(40) sds(12)\n",
      "  4. \n",
      ". * Generate metric independent variable #2\n",
      ". drawnorm x2, n(1000) means(200) sds(50)\n",
      "  5. \n",
      ". * Generate metric independent variable #3\n",
      ". drawnorm x3, n(1000) means(150) sds(5)\n",
      "  6. \n",
      ". * Generate metric dependent variable\n",
      ". gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n",
      "  7. \n",
      ". * Generate MAR\n",
      ". gen prob_mar = logistic(y-21791)\n",
      "  8. gen rmar = 0 if prob_mar==0\n",
      "  9. replace x2 = . if rmar == 0  // Set x to missing where rmar == 0\n",
      " 10. \n",
      ". summarize x2, meanonly\n",
      " 11. replace x2 = r(mean) if missing(x2)\n",
      " 12. \n",
      ". regress y x1 x2 x3\n",
      " 13. \n",
      ". gen beta1 = _b[x1]\n",
      " 14. gen se1 = _se[x1]\n",
      " 15. gen lower1 = beta1 - 1.96 * se1\n",
      " 16. gen upper1 = beta1 + 1.96 * se1\n",
      " 17. \n",
      ". gen beta2 = _b[x2]\n",
      " 18. gen se2 = _se[x2]\n",
      " 19. gen lower2 = beta2 - 1.96 * se2\n",
      " 20. gen upper2 = beta2 + 1.96 * se2\n",
      " 21. \n",
      ". gen beta3 = _b[x3]\n",
      " 22. gen se3 = _se[x3]\n",
      " 23. gen lower3 = beta3 - 1.96 * se3\n",
      " 24. gen upper3 = beta3 + 1.96 * se3\n",
      " 25. \n",
      ". \n",
      ". end\n",
      "\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "%%stata\n",
    "\n",
    "capture program drop missingmean\n",
    "\n",
    "program missingmean, rclass\n",
    " drop _all\n",
    "    \n",
    "* Set number of observations\n",
    "set obs 1000\n",
    "    \n",
    "* Generate metric independent variable #1\n",
    "drawnorm x1, n(1000) means(40) sds(12)\n",
    "\n",
    "* Generate metric independent variable #2\n",
    "drawnorm x2, n(1000) means(200) sds(50)\n",
    "\n",
    "* Generate metric independent variable #3\n",
    "drawnorm x3, n(1000) means(150) sds(5)\n",
    "\n",
    "* Generate metric dependent variable\n",
    "gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n",
    "\n",
    "* Generate MAR\n",
    "gen prob_mar = logistic(y-21791)\n",
    "gen rmar = 0 if prob_mar==0\n",
    "replace x2 = . if rmar == 0  // Set x to missing where rmar == 0\n",
    "\n",
    "summarize x2, meanonly\n",
    "replace x2 = r(mean) if missing(x2)\n",
    "\n",
    "regress y x1 x2 x3\n",
    "\n",
    "gen beta1 = _b[x1]\n",
    "gen se1 = _se[x1]\n",
    "gen lower1 = beta1 - 1.96 * se1\n",
    "gen upper1 = beta1 + 1.96 * se1\n",
    "\n",
    "gen beta2 = _b[x2]\n",
    "gen se2 = _se[x2]\n",
    "gen lower2 = beta2 - 1.96 * se2\n",
    "gen upper2 = beta2 + 1.96 * se2\n",
    "\n",
    "gen beta3 = _b[x3]\n",
    "gen se3 = _se[x3]\n",
    "gen lower3 = beta3 - 1.96 * se3\n",
    "gen upper3 = beta3 + 1.96 * se3\n",
    "\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e11ccf",
   "metadata": {},
   "source": [
    "The program \"semmlvm\" is the 'gold standard' Full Information Maximum Likelihood attempt to recover unbiased estimates following an injection of a MAR mechanism. FIML is performed using the sem command with the mlvm estimation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab0e26f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:25:28.439957Z",
     "start_time": "2025-04-05T10:25:28.401159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". \n",
      ". capture program drop semmlmv\n",
      "\n",
      ". \n",
      ". program semmlmv, rclass\n",
      "  1.     * drop all variables to create an empty dataset\n",
      ".     drop _all\n",
      "  2.     \n",
      ". * Set number of observations\n",
      ". set obs 1000\n",
      "  3.     \n",
      ". * Generate metric independent variable #1\n",
      ". drawnorm x1, n(1000) means(40) sds(12)\n",
      "  4. \n",
      ". * Generate metric independent variable #2\n",
      ". drawnorm x2, n(1000) means(200) sds(50)\n",
      "  5. \n",
      ". * Generate metric independent variable #3\n",
      ". drawnorm x3, n(1000) means(150) sds(5)\n",
      "  6. \n",
      ". * Generate metric dependent variable\n",
      ". gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n",
      "  7. \n",
      ". * Generate MAR\n",
      ". gen prob_mar = logistic(y-21791)\n",
      "  8. gen rmar = 0 if prob_mar==0\n",
      "  9. replace x2 = . if rmar == 0  // Set x to missing where rmar == 0\n",
      " 10.     \n",
      ". * Model estimation\n",
      ". sem (y <- x1 x2 x3), method(mlmv)\n",
      " 11. \n",
      ". gen beta1 = _b[x1]\n",
      " 12. gen se1 = _se[x1]\n",
      " 13. gen lower1 = beta1 - 1.96 * se1\n",
      " 14. gen upper1 = beta1 + 1.96 * se1\n",
      " 15. \n",
      ". gen beta2 = _b[x2]\n",
      " 16. gen se2 = _se[x2]\n",
      " 17. gen lower2 = beta2 - 1.96 * se2\n",
      " 18. gen upper2 = beta2 + 1.96 * se2\n",
      " 19. \n",
      ". gen beta3 = _b[x3]\n",
      " 20. gen se3 = _se[x3]\n",
      " 21. gen lower3 = beta3 - 1.96 * se3\n",
      " 22. gen upper3 = beta3 + 1.96 * se3\n",
      " 23.     \n",
      ". \n",
      ". end\n",
      "\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "%%stata\n",
    "\n",
    "capture program drop semmlmv\n",
    "\n",
    "program semmlmv, rclass\n",
    "    * drop all variables to create an empty dataset\n",
    "    drop _all\n",
    "    \n",
    "* Set number of observations\n",
    "set obs 1000\n",
    "    \n",
    "* Generate metric independent variable #1\n",
    "drawnorm x1, n(1000) means(40) sds(12)\n",
    "\n",
    "* Generate metric independent variable #2\n",
    "drawnorm x2, n(1000) means(200) sds(50)\n",
    "\n",
    "* Generate metric independent variable #3\n",
    "drawnorm x3, n(1000) means(150) sds(5)\n",
    "\n",
    "* Generate metric dependent variable\n",
    "gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n",
    "\n",
    "* Generate MAR\n",
    "gen prob_mar = logistic(y-21791)\n",
    "gen rmar = 0 if prob_mar==0\n",
    "replace x2 = . if rmar == 0  // Set x to missing where rmar == 0\n",
    "    \n",
    "* Model estimation\n",
    "sem (y <- x1 x2 x3), method(mlmv)\n",
    "\n",
    "gen beta1 = _b[x1]\n",
    "gen se1 = _se[x1]\n",
    "gen lower1 = beta1 - 1.96 * se1\n",
    "gen upper1 = beta1 + 1.96 * se1\n",
    "\n",
    "gen beta2 = _b[x2]\n",
    "gen se2 = _se[x2]\n",
    "gen lower2 = beta2 - 1.96 * se2\n",
    "gen upper2 = beta2 + 1.96 * se2\n",
    "\n",
    "gen beta3 = _b[x3]\n",
    "gen se3 = _se[x3]\n",
    "gen lower3 = beta3 - 1.96 * se3\n",
    "gen upper3 = beta3 + 1.96 * se3\n",
    "    \n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2db77f9",
   "metadata": {},
   "source": [
    "The program \"minoaux1\" performs multiple imputation with 10 iterations and 10 burnins with zero auxiliary variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53a639bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:25:29.407990Z",
     "start_time": "2025-04-05T10:25:29.369542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". \n",
      ". capture program drop minoaux1\n",
      "\n",
      ". \n",
      ". program minoaux1, rclass\n",
      "  1.     * drop all variables to create an empty dataset\n",
      ".     drop _all\n",
      "  2.     \n",
      ". * Set number of observations\n",
      ". set obs 1000\n",
      "  3.     \n",
      ". * Generate metric independent variable #1\n",
      ". drawnorm x1, n(1000) means(40) sds(12)\n",
      "  4. \n",
      ". * Generate metric independent variable #2\n",
      ". drawnorm x2, n(1000) means(200) sds(50)\n",
      "  5. \n",
      ". * Generate metric independent variable #3\n",
      ". drawnorm x3, n(1000) means(150) sds(5)\n",
      "  6. \n",
      ". * Generate metric dependent variable\n",
      ". gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n",
      "  7. \n",
      ". * Generate MAR\n",
      ". gen prob_mar = logistic(y-21791)\n",
      "  8. gen rmar = 0 if prob_mar==0\n",
      "  9. replace x2 = . if rmar == 0  // Set x to missing where rmar == 0\n",
      " 10.         \n",
      ".     * Model estimation\n",
      ". \n",
      ". mi set wide\n",
      " 11. \n",
      ". mi register imputed y x1 x2 x3\n",
      " 12. \n",
      ". tab _mi_miss\n",
      " 13. \n",
      ". \n",
      ". mi impute chained ///\n",
      "> ///\n",
      "> (regress) y x1 x2 x3 ///\n",
      "> , rseed(12345) dots force add(10) burnin(10) \n",
      " 14. \n",
      ". \n",
      ". mi estimate, post dots: regress y x1 x2 x3\n",
      " 15. \n",
      ". gen beta1 = _b[x1]\n",
      " 16. gen se1 = _se[x1]\n",
      " 17. gen lower1 = beta1 - 1.96 * se1\n",
      " 18. gen upper1 = beta1 + 1.96 * se1\n",
      " 19. \n",
      ". gen beta2 = _b[x2]\n",
      " 20. gen se2 = _se[x2]\n",
      " 21. gen lower2 = beta2 - 1.96 * se2\n",
      " 22. gen upper2 = beta2 + 1.96 * se2\n",
      " 23. \n",
      ". gen beta3 = _b[x3]\n",
      " 24. gen se3 = _se[x3]\n",
      " 25. gen lower3 = beta3 - 1.96 * se3\n",
      " 26. gen upper3 = beta3 + 1.96 * se3\n",
      " 27.     \n",
      ". \n",
      ". end\n",
      "\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "%%stata\n",
    "\n",
    "capture program drop minoaux1\n",
    "\n",
    "program minoaux1, rclass\n",
    "    * drop all variables to create an empty dataset\n",
    "    drop _all\n",
    "    \n",
    "* Set number of observations\n",
    "set obs 1000\n",
    "    \n",
    "* Generate metric independent variable #1\n",
    "drawnorm x1, n(1000) means(40) sds(12)\n",
    "\n",
    "* Generate metric independent variable #2\n",
    "drawnorm x2, n(1000) means(200) sds(50)\n",
    "\n",
    "* Generate metric independent variable #3\n",
    "drawnorm x3, n(1000) means(150) sds(5)\n",
    "\n",
    "* Generate metric dependent variable\n",
    "gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n",
    "\n",
    "* Generate MAR\n",
    "gen prob_mar = logistic(y-21791)\n",
    "gen rmar = 0 if prob_mar==0\n",
    "replace x2 = . if rmar == 0  // Set x to missing where rmar == 0\n",
    "\t\n",
    "    * Model estimation\n",
    "\n",
    "mi set wide\n",
    "\n",
    "mi register imputed y x1 x2 x3\n",
    "\n",
    "tab _mi_miss\n",
    "\n",
    "\n",
    "mi impute chained ///\n",
    "///\n",
    "(regress) y x1 x2 x3 ///\n",
    ", rseed(12345) dots force add(10) burnin(10) \n",
    "\n",
    "\n",
    "mi estimate, post dots: regress y x1 x2 x3\n",
    "\n",
    "gen beta1 = _b[x1]\n",
    "gen se1 = _se[x1]\n",
    "gen lower1 = beta1 - 1.96 * se1\n",
    "gen upper1 = beta1 + 1.96 * se1\n",
    "\n",
    "gen beta2 = _b[x2]\n",
    "gen se2 = _se[x2]\n",
    "gen lower2 = beta2 - 1.96 * se2\n",
    "gen upper2 = beta2 + 1.96 * se2\n",
    "\n",
    "gen beta3 = _b[x3]\n",
    "gen se3 = _se[x3]\n",
    "gen lower3 = beta3 - 1.96 * se3\n",
    "gen upper3 = beta3 + 1.96 * se3\n",
    "    \n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f0f923",
   "metadata": {},
   "source": [
    "The program \"miaux1\" performs multiple imputation with 10 iterations and 10 burnins with eight auxiliary variables. Five auxiliary variables are generated that are predictive of the probability of the missingness and the missing values, three are generated that are predictive of only the missing values themselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ea60bbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:25:30.549127Z",
     "start_time": "2025-04-05T10:25:30.509406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". \n",
      ". capture program drop miaux1\n",
      "\n",
      ". \n",
      ". \n",
      ". program miaux1, rclass\n",
      "  1.     * drop all variables to create an empty dataset\n",
      ".     drop _all\n",
      "  2.     \n",
      ". * Set number of observations\n",
      ". set obs 1000\n",
      "  3.     \n",
      ". * Generate metric independent variable #1\n",
      ". drawnorm x1, n(1000) means(40) sds(12)\n",
      "  4. \n",
      ". * Generate metric independent variable #2\n",
      ". drawnorm x2, n(1000) means(200) sds(50)\n",
      "  5. \n",
      ". * Generate metric independent variable #3\n",
      ". drawnorm x3, n(1000) means(150) sds(5)\n",
      "  6. \n",
      ". * Generate metric dependent variable\n",
      ". gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n",
      "  7. \n",
      ". * Generate MAR\n",
      ". gen prob_mar = logistic(y-21791)\n",
      "  8. gen rmar = 0 if prob_mar==0\n",
      "  9. replace x2 = . if rmar == 0  // Set x to missing where rmar == 0\n",
      " 10.         \n",
      ".         \n",
      ". * Generate Auxiliary Variables Predictive of Probability of Missingness and M\n",
      "> issing Values      \n",
      ". gen aux1 = 0.7*x2 + 0.3*y + rnormal(0, 10)   // Correlated with x2 and y\n",
      " 11. gen aux2 = 0.5*x2 + 0.5*y + rnormal(0, 15)   // Balanced correlation with \n",
      "> x2 and y\n",
      " 12. gen aux3 = 0.6*x2 + 0.2*x1 + 0.2*y + rnormal(0, 12)   // Adds x1 for sligh\n",
      "> t variation\n",
      " 13. gen aux4 = 0.4*x2 + 0.6*y + rnormal(0, 20)   // Heavier weight on y (missi\n",
      "> ngness predictor)\n",
      " 14. gen aux5 = 0.3*x2 + 0.3*y + 0.4*x3 + rnormal(0, 18)  // Introduces correla\n",
      "> tion with x3\n",
      " 15. \n",
      ". * Generate Auxiliary Variables Predictive of ONLY Missing Values \n",
      ". \n",
      ". gen aux6 = 0.9*x2 + rnormal(0, 10)   // Strong correlation with x2, no y depe\n",
      "> ndence\n",
      " 16. gen aux7 = 0.8*x2 + 0.2*x3 + rnormal(0, 12)   // Includes x3, but no direc\n",
      "> t link to missingness\n",
      " 17. gen aux8 = 0.85*x2 + 0.1*x1 + rnormal(0, 15)  // Includes x1, but missingn\n",
      "> ess-independent\n",
      " 18.         \n",
      ". * setting MI data up\n",
      ". \n",
      ". mi set wide\n",
      " 19. \n",
      ". mi register imputed y x1 x2 x3 aux1 aux2 aux3 aux4 aux5 aux6 aux7 aux8 \n",
      " 20. \n",
      ". \n",
      ". mi impute chained ///\n",
      "> ///\n",
      "> (regress) y x1 x2 x3 aux1 aux2 aux3 aux4 aux5 aux6 aux7 aux8 ///\n",
      "> , rseed(12345) dots force add(10) burnin(10) \n",
      " 21. \n",
      ". \n",
      ". * mi model\n",
      ". \n",
      ". mi estimate, post dots: regress y x1 x2 x3\n",
      " 22. \n",
      ". gen beta1 = _b[x1]\n",
      " 23. gen se1 = _se[x1]\n",
      " 24. gen lower1 = beta1 - 1.96 * se1\n",
      " 25. gen upper1 = beta1 + 1.96 * se1\n",
      " 26. \n",
      ". gen beta2 = _b[x2]\n",
      " 27. gen se2 = _se[x2]\n",
      " 28. gen lower2 = beta2 - 1.96 * se2\n",
      " 29. gen upper2 = beta2 + 1.96 * se2\n",
      " 30. \n",
      ". gen beta3 = _b[x3]\n",
      " 31. gen se3 = _se[x3]\n",
      " 32. gen lower3 = beta3 - 1.96 * se3\n",
      " 33. gen upper3 = beta3 + 1.96 * se3\n",
      " 34.     \n",
      ". \n",
      ". end\n",
      "\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "%%stata\n",
    "\n",
    "capture program drop miaux1\n",
    "\n",
    "\n",
    "program miaux1, rclass\n",
    "    * drop all variables to create an empty dataset\n",
    "    drop _all\n",
    "    \n",
    "* Set number of observations\n",
    "set obs 1000\n",
    "    \n",
    "* Generate metric independent variable #1\n",
    "drawnorm x1, n(1000) means(40) sds(12)\n",
    "\n",
    "* Generate metric independent variable #2\n",
    "drawnorm x2, n(1000) means(200) sds(50)\n",
    "\n",
    "* Generate metric independent variable #3\n",
    "drawnorm x3, n(1000) means(150) sds(5)\n",
    "\n",
    "* Generate metric dependent variable\n",
    "gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n",
    "\n",
    "* Generate MAR\n",
    "gen prob_mar = logistic(y-21791)\n",
    "gen rmar = 0 if prob_mar==0\n",
    "replace x2 = . if rmar == 0  // Set x to missing where rmar == 0\n",
    "\t\n",
    "\t\n",
    "* Generate Auxiliary Variables Predictive of Probability of Missingness and Missing Values \t\n",
    "gen aux1 = 0.7*x2 + 0.3*y + rnormal(0, 10)   // Correlated with x2 and y\n",
    "gen aux2 = 0.5*x2 + 0.5*y + rnormal(0, 15)   // Balanced correlation with x2 and y\n",
    "gen aux3 = 0.6*x2 + 0.2*x1 + 0.2*y + rnormal(0, 12)   // Adds x1 for slight variation\n",
    "gen aux4 = 0.4*x2 + 0.6*y + rnormal(0, 20)   // Heavier weight on y (missingness predictor)\n",
    "gen aux5 = 0.3*x2 + 0.3*y + 0.4*x3 + rnormal(0, 18)  // Introduces correlation with x3\n",
    "\n",
    "* Generate Auxiliary Variables Predictive of ONLY Missing Values \n",
    "\n",
    "gen aux6 = 0.9*x2 + rnormal(0, 10)   // Strong correlation with x2, no y dependence\n",
    "gen aux7 = 0.8*x2 + 0.2*x3 + rnormal(0, 12)   // Includes x3, but no direct link to missingness\n",
    "gen aux8 = 0.85*x2 + 0.1*x1 + rnormal(0, 15)  // Includes x1, but missingness-independent\n",
    "\t\n",
    "* setting MI data up\n",
    "\n",
    "mi set wide\n",
    "\n",
    "mi register imputed y x1 x2 x3 aux1 aux2 aux3 aux4 aux5 aux6 aux7 aux8 \n",
    "\n",
    "\n",
    "mi impute chained ///\n",
    "///\n",
    "(regress) y x1 x2 x3 aux1 aux2 aux3 aux4 aux5 aux6 aux7 aux8 ///\n",
    ", rseed(12345) dots force add(10) burnin(10) \n",
    "\n",
    "\n",
    "* mi model\n",
    "\n",
    "mi estimate, post dots: regress y x1 x2 x3\n",
    "\n",
    "gen beta1 = _b[x1]\n",
    "gen se1 = _se[x1]\n",
    "gen lower1 = beta1 - 1.96 * se1\n",
    "gen upper1 = beta1 + 1.96 * se1\n",
    "\n",
    "gen beta2 = _b[x2]\n",
    "gen se2 = _se[x2]\n",
    "gen lower2 = beta2 - 1.96 * se2\n",
    "gen upper2 = beta2 + 1.96 * se2\n",
    "\n",
    "gen beta3 = _b[x3]\n",
    "gen se3 = _se[x3]\n",
    "gen lower3 = beta3 - 1.96 * se3\n",
    "gen upper3 = beta3 + 1.96 * se3\n",
    "    \n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef7713c",
   "metadata": {},
   "source": [
    "The final program \"miaux100\" performs multiple imputation with 100 iterations and 20 burnin using the prior generated eight auxiliary variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b1ef5ccb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:25:31.537157Z",
     "start_time": "2025-04-05T10:25:31.498974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". \n",
      ". capture program drop miaux100\n",
      "\n",
      ". \n",
      ". \n",
      ". program miaux100, rclass\n",
      "  1.     * drop all variables to create an empty dataset\n",
      ".     drop _all\n",
      "  2.     \n",
      ". * Set number of observations\n",
      ". set obs 1000\n",
      "  3.     \n",
      ". * Generate metric independent variable #1\n",
      ". drawnorm x1, n(1000) means(40) sds(12)\n",
      "  4. \n",
      ". * Generate metric independent variable #2\n",
      ". drawnorm x2, n(1000) means(200) sds(50)\n",
      "  5. \n",
      ". * Generate metric independent variable #3\n",
      ". drawnorm x3, n(1000) means(150) sds(5)\n",
      "  6. \n",
      ". * Generate metric dependent variable\n",
      ". gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n",
      "  7. \n",
      ". * Generate MAR\n",
      ". gen prob_mar = logistic(y-21791)\n",
      "  8. gen rmar = 0 if prob_mar==0\n",
      "  9. replace x2 = . if rmar == 0  // Set x to missing where rmar == 0\n",
      " 10.         \n",
      ".         \n",
      ". * Generate Auxiliary Variables Predictive of Probability of Missingness and M\n",
      "> issing Values      \n",
      ". gen aux1 = 0.7*x2 + 0.3*y + rnormal(0, 10)   // Correlated with x2 and y\n",
      " 11. gen aux2 = 0.5*x2 + 0.5*y + rnormal(0, 15)   // Balanced correlation with \n",
      "> x2 and y\n",
      " 12. gen aux3 = 0.6*x2 + 0.2*x1 + 0.2*y + rnormal(0, 12)   // Adds x1 for sligh\n",
      "> t variation\n",
      " 13. gen aux4 = 0.4*x2 + 0.6*y + rnormal(0, 20)   // Heavier weight on y (missi\n",
      "> ngness predictor)\n",
      " 14. gen aux5 = 0.3*x2 + 0.3*y + 0.4*x3 + rnormal(0, 18)  // Introduces correla\n",
      "> tion with x3\n",
      " 15. \n",
      ". * Generate Auxiliary Variables Predictive of ONLY Missing Values \n",
      ". \n",
      ". gen aux6 = 0.9*x2 + rnormal(0, 10)   // Strong correlation with x2, no y depe\n",
      "> ndence\n",
      " 16. gen aux7 = 0.8*x2 + 0.2*x3 + rnormal(0, 12)   // Includes x3, but no direc\n",
      "> t link to missingness\n",
      " 17. gen aux8 = 0.85*x2 + 0.1*x1 + rnormal(0, 15)  // Includes x1, but missingn\n",
      "> ess-independent\n",
      " 18.         \n",
      ". * setting MI data up\n",
      ". \n",
      ". mi set wide\n",
      " 19. \n",
      ". mi register imputed y x1 x2 x3 aux1 aux2 aux3 aux4 aux5 aux6 aux7 aux8 \n",
      " 20. \n",
      ". \n",
      ". mi impute chained ///\n",
      "> ///\n",
      "> (regress) y x1 x2 x3 aux1 aux2 aux3 aux4 aux5 aux6 aux7 aux8 ///\n",
      "> , rseed(12345) dots force add(100) burnin(20) \n",
      " 21. \n",
      ". \n",
      ". * mi model\n",
      ". \n",
      ". mi estimate, post dots: regress y x1 x2 x3\n",
      " 22. \n",
      ". gen beta1 = _b[x1]\n",
      " 23. gen se1 = _se[x1]\n",
      " 24. gen lower1 = beta1 - 1.96 * se1\n",
      " 25. gen upper1 = beta1 + 1.96 * se1\n",
      " 26. \n",
      ". gen beta2 = _b[x2]\n",
      " 27. gen se2 = _se[x2]\n",
      " 28. gen lower2 = beta2 - 1.96 * se2\n",
      " 29. gen upper2 = beta2 + 1.96 * se2\n",
      " 30. \n",
      ". gen beta3 = _b[x3]\n",
      " 31. gen se3 = _se[x3]\n",
      " 32. gen lower3 = beta3 - 1.96 * se3\n",
      " 33. gen upper3 = beta3 + 1.96 * se3   \n",
      " 34. \n",
      ". end\n",
      "\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "%%stata\n",
    "\n",
    "capture program drop miaux100\n",
    "\n",
    "\n",
    "program miaux100, rclass\n",
    "    * drop all variables to create an empty dataset\n",
    "    drop _all\n",
    "    \n",
    "* Set number of observations\n",
    "set obs 1000\n",
    "    \n",
    "* Generate metric independent variable #1\n",
    "drawnorm x1, n(1000) means(40) sds(12)\n",
    "\n",
    "* Generate metric independent variable #2\n",
    "drawnorm x2, n(1000) means(200) sds(50)\n",
    "\n",
    "* Generate metric independent variable #3\n",
    "drawnorm x3, n(1000) means(150) sds(5)\n",
    "\n",
    "* Generate metric dependent variable\n",
    "gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n",
    "\n",
    "* Generate MAR\n",
    "gen prob_mar = logistic(y-21791)\n",
    "gen rmar = 0 if prob_mar==0\n",
    "replace x2 = . if rmar == 0  // Set x to missing where rmar == 0\n",
    "\t\n",
    "\t\n",
    "* Generate Auxiliary Variables Predictive of Probability of Missingness and Missing Values \t\n",
    "gen aux1 = 0.7*x2 + 0.3*y + rnormal(0, 10)   // Correlated with x2 and y\n",
    "gen aux2 = 0.5*x2 + 0.5*y + rnormal(0, 15)   // Balanced correlation with x2 and y\n",
    "gen aux3 = 0.6*x2 + 0.2*x1 + 0.2*y + rnormal(0, 12)   // Adds x1 for slight variation\n",
    "gen aux4 = 0.4*x2 + 0.6*y + rnormal(0, 20)   // Heavier weight on y (missingness predictor)\n",
    "gen aux5 = 0.3*x2 + 0.3*y + 0.4*x3 + rnormal(0, 18)  // Introduces correlation with x3\n",
    "\n",
    "* Generate Auxiliary Variables Predictive of ONLY Missing Values \n",
    "\n",
    "gen aux6 = 0.9*x2 + rnormal(0, 10)   // Strong correlation with x2, no y dependence\n",
    "gen aux7 = 0.8*x2 + 0.2*x3 + rnormal(0, 12)   // Includes x3, but no direct link to missingness\n",
    "gen aux8 = 0.85*x2 + 0.1*x1 + rnormal(0, 15)  // Includes x1, but missingness-independent\n",
    "\t\n",
    "* setting MI data up\n",
    "\n",
    "mi set wide\n",
    "\n",
    "mi register imputed y x1 x2 x3 aux1 aux2 aux3 aux4 aux5 aux6 aux7 aux8 \n",
    "\n",
    "\n",
    "mi impute chained ///\n",
    "///\n",
    "(regress) y x1 x2 x3 aux1 aux2 aux3 aux4 aux5 aux6 aux7 aux8 ///\n",
    ", rseed(12345) dots force add(100) burnin(20) \n",
    "\n",
    "\n",
    "* mi model\n",
    "\n",
    "mi estimate, post dots: regress y x1 x2 x3\n",
    "\n",
    "gen beta1 = _b[x1]\n",
    "gen se1 = _se[x1]\n",
    "gen lower1 = beta1 - 1.96 * se1\n",
    "gen upper1 = beta1 + 1.96 * se1\n",
    "\n",
    "gen beta2 = _b[x2]\n",
    "gen se2 = _se[x2]\n",
    "gen lower2 = beta2 - 1.96 * se2\n",
    "gen upper2 = beta2 + 1.96 * se2\n",
    "\n",
    "gen beta3 = _b[x3]\n",
    "gen se3 = _se[x3]\n",
    "gen lower3 = beta3 - 1.96 * se3\n",
    "gen upper3 = beta3 + 1.96 * se3   \n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d4232f",
   "metadata": {},
   "source": [
    "This next section of the Notebook now runs each program simulation. Each simulation is run 1000 times with a set seed of \"1234\" for each program simulation. The 95 per cent confidence intervals of the means for each beta and standard error within the models is extracted and the dataset is then saved for each simulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ffac7b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:25:43.464071Z",
     "start_time": "2025-04-05T10:25:33.769859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". \n",
      ". simulate beta1 se1 beta2 se2 beta3 se3 lower1 lower2 lower3 upper1 upper2 upp\n",
      "> er3, reps(1000) seed(1234): regress1 \n",
      "\n",
      "      Command: regress1\n",
      "       _sim_1: beta1\n",
      "       _sim_2: se1\n",
      "       _sim_3: beta2\n",
      "       _sim_4: se2\n",
      "       _sim_5: beta3\n",
      "       _sim_6: se3\n",
      "       _sim_7: lower1\n",
      "       _sim_8: lower2\n",
      "       _sim_9: lower3\n",
      "      _sim_10: upper1\n",
      "      _sim_11: upper2\n",
      "      _sim_12: upper3\n",
      "\n",
      "Simulations (1,000): .........10.........20.........30.........40.........50...\n",
      "> ......60.........70.........80.........90.........100.........110.........120\n",
      "> .........130.........140.........150.........160.........170.........180.....\n",
      "> ....190.........200.........210.........220.........230.........240.........2\n",
      "> 50.........260.........270.........280.........290.........300.........310...\n",
      "> ......320.........330.........340.........350.........360.........370........\n",
      "> .380.........390.........400.........410.........420.........430.........440.\n",
      "> ........450.........460.........470.........480.........490.........500......\n",
      "> ...510.........520.........530.........540.........550.........560.........57\n",
      "> 0.........580.........590.........600.........610.........620.........630....\n",
      "> .....640.........650.........660.........670.........680.........690.........\n",
      "> 700.........710.........720.........730.........740.........750.........760..\n",
      "> .......770.........780.........790.........800.........810.........820.......\n",
      "> ..830.........840.........850.........860.........870.........880.........890\n",
      "> .........900.........910.........920.........930.........940.........950.....\n",
      "> ....960.........970.........980.........990.........1,000 done\n",
      "\n",
      ". \n",
      ". ci mean _sim_1 _sim_2 _sim_3 _sim_4 _sim_5 _sim_6 _sim_7 _sim_8 _sim_9 _sim_1\n",
      "> 0 _sim_11 _sim_12\n",
      "\n",
      "    Variable |        Obs        Mean    Std. err.       [95% conf. interval]\n",
      "-------------+---------------------------------------------------------------\n",
      "      _sim_1 |      1,000    30.00808    .1223364        29.76802    30.24815\n",
      "      _sim_2 |      1,000    3.958464    .0039263        3.950759    3.966169\n",
      "      _sim_3 |      1,000    40.01876    .0299956         39.9599    40.07762\n",
      "      _sim_4 |      1,000    .9516209    .0009688        .9497198    .9535221\n",
      "      _sim_5 |      1,000    49.87959    .3001486        49.29059    50.46858\n",
      "-------------+---------------------------------------------------------------\n",
      "      _sim_6 |      1,000     9.51589    .0095674        9.497115    9.534664\n",
      "      _sim_7 |      1,000    22.24949    .1221543        22.00978     22.4892\n",
      "      _sim_8 |      1,000    38.15358    .0300231        38.09467     38.2125\n",
      "      _sim_9 |      1,000    31.22844    .2998234        30.64009     31.8168\n",
      "     _sim_10 |      1,000    37.76667    .1230006         37.5253    38.00804\n",
      "-------------+---------------------------------------------------------------\n",
      "     _sim_11 |      1,000    41.88393    .0300882        41.82489    41.94298\n",
      "     _sim_12 |      1,000    68.53073    .3016415        67.93881    69.12265\n",
      "\n",
      ". \n",
      ". rename _sim_1 beta1a \n",
      "\n",
      ". rename _sim_2 se1a \n",
      "\n",
      ". rename _sim_3 beta2a \n",
      "\n",
      ". rename _sim_4 se2a \n",
      "\n",
      ". rename _sim_5 beta3a \n",
      "\n",
      ". rename _sim_6 se3a \n",
      "\n",
      ". rename _sim_7 ll1a \n",
      "\n",
      ". rename _sim_8 ll2a \n",
      "\n",
      ". rename _sim_9 ll3a \n",
      "\n",
      ". rename _sim_10 ul1a \n",
      "\n",
      ". rename _sim_11 ul2a \n",
      "\n",
      ". rename _sim_12 ul3a \n",
      "\n",
      ". \n",
      ". \n",
      ". save godmodel, replace\n",
      "file godmodel.dta saved\n",
      "\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "%%stata \n",
    "\n",
    "simulate beta1 se1 beta2 se2 beta3 se3 lower1 lower2 lower3 upper1 upper2 upper3, reps(1000) seed(1234): regress1 \n",
    "\n",
    "ci mean _sim_1 _sim_2 _sim_3 _sim_4 _sim_5 _sim_6 _sim_7 _sim_8 _sim_9 _sim_10 _sim_11 _sim_12\n",
    "\n",
    "rename _sim_1 beta1a \n",
    "rename _sim_2 se1a \n",
    "rename _sim_3 beta2a \n",
    "rename _sim_4 se2a \n",
    "rename _sim_5 beta3a \n",
    "rename _sim_6 se3a \n",
    "rename _sim_7 ll1a \n",
    "rename _sim_8 ll2a \n",
    "rename _sim_9 ll3a \n",
    "rename _sim_10 ul1a \n",
    "rename _sim_11 ul2a \n",
    "rename _sim_12 ul3a \n",
    "\n",
    "\n",
    "save godmodel, replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e981272",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:26:27.798173Z",
     "start_time": "2025-04-05T10:26:01.489356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". \n",
      ". simulate beta1 se1 beta2 se2 beta3 se3 lower1 lower2 lower3 upper1 upper2 upp\n",
      "> er3, reps(1000) seed(1234): sem1 \n",
      "\n",
      "      Command: sem1\n",
      "       _sim_1: beta1\n",
      "       _sim_2: se1\n",
      "       _sim_3: beta2\n",
      "       _sim_4: se2\n",
      "       _sim_5: beta3\n",
      "       _sim_6: se3\n",
      "       _sim_7: lower1\n",
      "       _sim_8: lower2\n",
      "       _sim_9: lower3\n",
      "      _sim_10: upper1\n",
      "      _sim_11: upper2\n",
      "      _sim_12: upper3\n",
      "\n",
      "Simulations (1,000): .........10.........20.........30.........40.........50...\n",
      "> ......60.........70.........80.........90.........100.........110.........120\n",
      "> .........130.........140.........150.........160.........170.........180.....\n",
      "> ....190.........200.........210.........220.........230.........240.........2\n",
      "> 50.........260.........270.........280.........290.........300.........310...\n",
      "> ......320.........330.........340.........350.........360.........370........\n",
      "> .380.........390.........400.........410.........420.........430.........440.\n",
      "> ........450.........460.........470.........480.........490.........500......\n",
      "> ...510.........520.........530.........540.........550.........560.........57\n",
      "> 0.........580.........590.........600.........610.........620.........630....\n",
      "> .....640.........650.........660.........670.........680.........690.........\n",
      "> 700.........710.........720.........730.........740.........750.........760..\n",
      "> .......770.........780.........790.........800.........810.........820.......\n",
      "> ..830.........840.........850.........860.........870.........880.........890\n",
      "> .........900.........910.........920.........930.........940.........950.....\n",
      "> ....960.........970.........980.........990.........1,000 done\n",
      "\n",
      ". \n",
      ". ci mean _sim_1 _sim_2 _sim_3 _sim_4 _sim_5 _sim_6 _sim_7 _sim_8 _sim_9 _sim_1\n",
      "> 0 _sim_11 _sim_12\n",
      "\n",
      "    Variable |        Obs        Mean    Std. err.       [95% conf. interval]\n",
      "-------------+---------------------------------------------------------------\n",
      "      _sim_1 |      1,000    30.00808    .1223364        29.76802    30.24815\n",
      "      _sim_2 |      1,000    3.950539    .0039185         3.94285    3.958229\n",
      "      _sim_3 |      1,000    40.01876    .0299956         39.9599    40.07762\n",
      "      _sim_4 |      1,000    .9497158    .0009669        .9478184    .9516131\n",
      "      _sim_5 |      1,000    49.87959    .3001486        49.29059    50.46858\n",
      "-------------+---------------------------------------------------------------\n",
      "      _sim_6 |      1,000    9.496839    .0095483        9.478102    9.515576\n",
      "      _sim_7 |      1,000    22.26502    .1221542        22.02532    22.50473\n",
      "      _sim_8 |      1,000    38.15731    .0300229         38.0984    38.21623\n",
      "      _sim_9 |      1,000    31.26578    .2998229        30.67743    31.85414\n",
      "     _sim_10 |      1,000    37.75114    .1229988        37.50977     37.9925\n",
      "-------------+---------------------------------------------------------------\n",
      "     _sim_11 |      1,000     41.8802    .0300878        41.82116    41.93924\n",
      "     _sim_12 |      1,000    68.49339    .3016373        67.90147    69.08531\n",
      "\n",
      ". \n",
      ". rename _sim_1 beta1b\n",
      "\n",
      ". rename _sim_2 se1b\n",
      "\n",
      ". rename _sim_3 beta2b \n",
      "\n",
      ". rename _sim_4 se2b\n",
      "\n",
      ". rename _sim_5 beta3b \n",
      "\n",
      ". rename _sim_6 se3b\n",
      "\n",
      ". rename _sim_7 ll1b \n",
      "\n",
      ". rename _sim_8 ll2b \n",
      "\n",
      ". rename _sim_9 ll3b \n",
      "\n",
      ". rename _sim_10 ul1b \n",
      "\n",
      ". rename _sim_11 ul2b \n",
      "\n",
      ". rename _sim_12 ul3b \n",
      "\n",
      ". \n",
      ". save semgodmodel, replace\n",
      "file semgodmodel.dta saved\n",
      "\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "%%stata \n",
    "\n",
    "simulate beta1 se1 beta2 se2 beta3 se3 lower1 lower2 lower3 upper1 upper2 upper3, reps(1000) seed(1234): sem1 \n",
    "\n",
    "ci mean _sim_1 _sim_2 _sim_3 _sim_4 _sim_5 _sim_6 _sim_7 _sim_8 _sim_9 _sim_10 _sim_11 _sim_12\n",
    "\n",
    "rename _sim_1 beta1b\n",
    "rename _sim_2 se1b\n",
    "rename _sim_3 beta2b \n",
    "rename _sim_4 se2b\n",
    "rename _sim_5 beta3b \n",
    "rename _sim_6 se3b\n",
    "rename _sim_7 ll1b \n",
    "rename _sim_8 ll2b \n",
    "rename _sim_9 ll3b \n",
    "rename _sim_10 ul1b \n",
    "rename _sim_11 ul2b \n",
    "rename _sim_12 ul3b \n",
    "\n",
    "save semgodmodel, replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac4eea06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:26:37.845652Z",
     "start_time": "2025-04-05T10:26:27.800576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". \n",
      ". simulate beta1 se1 beta2 se2 beta3 se3 lower1 lower2 lower3 upper1 upper2 upp\n",
      "> er3, reps(1000) seed(1234): missingmcar \n",
      "\n",
      "      Command: missingmcar\n",
      "       _sim_1: beta1\n",
      "       _sim_2: se1\n",
      "       _sim_3: beta2\n",
      "       _sim_4: se2\n",
      "       _sim_5: beta3\n",
      "       _sim_6: se3\n",
      "       _sim_7: lower1\n",
      "       _sim_8: lower2\n",
      "       _sim_9: lower3\n",
      "      _sim_10: upper1\n",
      "      _sim_11: upper2\n",
      "      _sim_12: upper3\n",
      "\n",
      "Simulations (1,000): .........10.........20.........30.........40.........50...\n",
      "> ......60.........70.........80.........90.........100.........110.........120\n",
      "> .........130.........140.........150.........160.........170.........180.....\n",
      "> ....190.........200.........210.........220.........230.........240.........2\n",
      "> 50.........260.........270.........280.........290.........300.........310...\n",
      "> ......320.........330.........340.........350.........360.........370........\n",
      "> .380.........390.........400.........410.........420.........430.........440.\n",
      "> ........450.........460.........470.........480.........490.........500......\n",
      "> ...510.........520.........530.........540.........550.........560.........57\n",
      "> 0.........580.........590.........600.........610.........620.........630....\n",
      "> .....640.........650.........660.........670.........680.........690.........\n",
      "> 700.........710.........720.........730.........740.........750.........760..\n",
      "> .......770.........780.........790.........800.........810.........820.......\n",
      "> ..830.........840.........850.........860.........870.........880.........890\n",
      "> .........900.........910.........920.........930.........940.........950.....\n",
      "> ....960.........970.........980.........990.........1,000 done\n",
      "\n",
      ".     \n",
      ". ci mean _sim_1 _sim_2 _sim_3 _sim_4 _sim_5 _sim_6 _sim_7 _sim_8 _sim_9 _sim_1\n",
      "> 0 _sim_11 _sim_12\n",
      "\n",
      "    Variable |        Obs        Mean    Std. err.       [95% conf. interval]\n",
      "-------------+---------------------------------------------------------------\n",
      "      _sim_1 |      1,000    29.96598    .1732293        29.62604    30.30591\n",
      "      _sim_2 |      1,000    5.622786    .0085991        5.605912     5.63966\n",
      "      _sim_3 |      1,000    40.02882    .0423628        39.94568    40.11195\n",
      "      _sim_4 |      1,000    1.348911     .002079        1.344832    1.352991\n",
      "      _sim_5 |      1,000    51.29684     .436001        50.44125    52.15242\n",
      "-------------+---------------------------------------------------------------\n",
      "      _sim_6 |      1,000    13.47856    .0213482        13.43667    13.52046\n",
      "      _sim_7 |      1,000    18.94532    .1742216        18.60344     19.2872\n",
      "      _sim_8 |      1,000    37.38495     .042557        37.30144    37.46846\n",
      "      _sim_9 |      1,000    24.87885    .4352299        24.02478    25.73292\n",
      "     _sim_10 |      1,000    40.98664    .1738728        40.64544    41.32784\n",
      "-------------+---------------------------------------------------------------\n",
      "     _sim_11 |      1,000    42.67268    .0425597        42.58916     42.7562\n",
      "     _sim_12 |      1,000    77.71482     .440761         76.8499    78.57974\n",
      "\n",
      ". \n",
      ". rename _sim_1 beta1c\n",
      "\n",
      ". rename _sim_2 se1c\n",
      "\n",
      ". rename _sim_3 beta2c \n",
      "\n",
      ". rename _sim_4 se2c\n",
      "\n",
      ". rename _sim_5 beta3c \n",
      "\n",
      ". rename _sim_6 se3c\n",
      "\n",
      ". rename _sim_7 ll1c \n",
      "\n",
      ". rename _sim_8 ll2c \n",
      "\n",
      ". rename _sim_9 ll3c \n",
      "\n",
      ". rename _sim_10 ul1c \n",
      "\n",
      ". rename _sim_11 ul2c \n",
      "\n",
      ". rename _sim_12 ul3c \n",
      "\n",
      ". \n",
      ". save missingmcar, replace\n",
      "file missingmcar.dta saved\n",
      "\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "%%stata\n",
    "\n",
    "simulate beta1 se1 beta2 se2 beta3 se3 lower1 lower2 lower3 upper1 upper2 upper3, reps(1000) seed(1234): missingmcar \n",
    "    \n",
    "ci mean _sim_1 _sim_2 _sim_3 _sim_4 _sim_5 _sim_6 _sim_7 _sim_8 _sim_9 _sim_10 _sim_11 _sim_12\n",
    "\n",
    "rename _sim_1 beta1c\n",
    "rename _sim_2 se1c\n",
    "rename _sim_3 beta2c \n",
    "rename _sim_4 se2c\n",
    "rename _sim_5 beta3c \n",
    "rename _sim_6 se3c\n",
    "rename _sim_7 ll1c \n",
    "rename _sim_8 ll2c \n",
    "rename _sim_9 ll3c \n",
    "rename _sim_10 ul1c \n",
    "rename _sim_11 ul2c \n",
    "rename _sim_12 ul3c \n",
    "\n",
    "save missingmcar, replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f0a67033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:27:07.166331Z",
     "start_time": "2025-04-05T10:26:56.710825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". \n",
      ". simulate beta1 se1 beta2 se2 beta3 se3 lower1 lower2 lower3 upper1 upper2 upp\n",
      "> er3, reps(1000) seed(1234): missingmar \n",
      "\n",
      "      Command: missingmar\n",
      "       _sim_1: beta1\n",
      "       _sim_2: se1\n",
      "       _sim_3: beta2\n",
      "       _sim_4: se2\n",
      "       _sim_5: beta3\n",
      "       _sim_6: se3\n",
      "       _sim_7: lower1\n",
      "       _sim_8: lower2\n",
      "       _sim_9: lower3\n",
      "      _sim_10: upper1\n",
      "      _sim_11: upper2\n",
      "      _sim_12: upper3\n",
      "\n",
      "Simulations (1,000): .........10.........20.........30.........40.........50...\n",
      "> ......60.........70.........80.........90.........100.........110.........120\n",
      "> .........130.........140.........150.........160.........170.........180.....\n",
      "> ....190.........200.........210.........220.........230.........240.........2\n",
      "> 50.........260.........270.........280.........290.........300.........310...\n",
      "> ......320.........330.........340.........350.........360.........370........\n",
      "> .380.........390.........400.........410.........420.........430.........440.\n",
      "> ........450.........460.........470.........480.........490.........500......\n",
      "> ...510.........520.........530.........540.........550.........560.........57\n",
      "> 0.........580.........590.........600.........610.........620.........630....\n",
      "> .....640.........650.........660.........670.........680.........690.........\n",
      "> 700.........710.........720.........730.........740.........750.........760..\n",
      "> .......770.........780.........790.........800.........810.........820.......\n",
      "> ..830.........840.........850.........860.........870.........880.........890\n",
      "> .........900.........910.........920.........930.........940.........950.....\n",
      "> ....960.........970.........980.........990.........1,000 done\n",
      "\n",
      ". \n",
      ". ci mean _sim_1 _sim_2 _sim_3 _sim_4 _sim_5 _sim_6 _sim_7 _sim_8 _sim_9 _sim_1\n",
      "> 0 _sim_11 _sim_12\n",
      "\n",
      "    Variable |        Obs        Mean    Std. err.       [95% conf. interval]\n",
      "-------------+---------------------------------------------------------------\n",
      "      _sim_1 |      1,000    18.43474    .1438892        18.15238     18.7171\n",
      "      _sim_2 |      1,000    4.470689    .0070771        4.456801    4.484577\n",
      "      _sim_3 |      1,000    24.75613    .0472162        24.66347    24.84878\n",
      "      _sim_4 |      1,000    1.375226    .0021105        1.371085    1.379368\n",
      "      _sim_5 |      1,000    30.23444     .337308        29.57253    30.89636\n",
      "-------------+---------------------------------------------------------------\n",
      "      _sim_6 |      1,000    10.68736    .0160744        10.65581     10.7189\n",
      "      _sim_7 |      1,000    9.672187    .1436357        9.390325     9.95405\n",
      "      _sim_8 |      1,000    22.06068    .0473267        21.96781    22.15355\n",
      "      _sim_9 |      1,000    9.287222    .3390662        8.621858    9.952586\n",
      "     _sim_10 |      1,000    27.19729     .145471        26.91183    27.48275\n",
      "-------------+---------------------------------------------------------------\n",
      "     _sim_11 |      1,000    27.45157    .0474674        27.35842    27.54472\n",
      "     _sim_12 |      1,000    51.18166    .3384859        50.51744    51.84589\n",
      "\n",
      ". \n",
      ". rename _sim_1 beta1d\n",
      "\n",
      ". rename _sim_2 se1d\n",
      "\n",
      ". rename _sim_3 beta2d \n",
      "\n",
      ". rename _sim_4 se2d\n",
      "\n",
      ". rename _sim_5 beta3d \n",
      "\n",
      ". rename _sim_6 se3d\n",
      "\n",
      ". rename _sim_7 ll1d \n",
      "\n",
      ". rename _sim_8 ll2d \n",
      "\n",
      ". rename _sim_9 ll3d \n",
      "\n",
      ". rename _sim_10 ul1d \n",
      "\n",
      ". rename _sim_11 ul2d \n",
      "\n",
      ". rename _sim_12 ul3d \n",
      "\n",
      ". \n",
      ". save missingmar, replace\n",
      "file missingmar.dta saved\n",
      "\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "%%stata\n",
    "\n",
    "simulate beta1 se1 beta2 se2 beta3 se3 lower1 lower2 lower3 upper1 upper2 upper3, reps(1000) seed(1234): missingmar \n",
    "\n",
    "ci mean _sim_1 _sim_2 _sim_3 _sim_4 _sim_5 _sim_6 _sim_7 _sim_8 _sim_9 _sim_10 _sim_11 _sim_12\n",
    "\n",
    "rename _sim_1 beta1d\n",
    "rename _sim_2 se1d\n",
    "rename _sim_3 beta2d \n",
    "rename _sim_4 se2d\n",
    "rename _sim_5 beta3d \n",
    "rename _sim_6 se3d\n",
    "rename _sim_7 ll1d \n",
    "rename _sim_8 ll2d \n",
    "rename _sim_9 ll3d \n",
    "rename _sim_10 ul1d \n",
    "rename _sim_11 ul2d \n",
    "rename _sim_12 ul3d \n",
    "\n",
    "save missingmar, replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2a5f499f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:27:25.902148Z",
     "start_time": "2025-04-05T10:27:15.290697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". \n",
      ". simulate beta1 se1 beta2 se2 beta3 se3 lower1 lower2 lower3 upper1 upper2 upp\n",
      "> er3, reps(1000) seed(1234): missingmean \n",
      "\n",
      "      Command: missingmean\n",
      "       _sim_1: beta1\n",
      "       _sim_2: se1\n",
      "       _sim_3: beta2\n",
      "       _sim_4: se2\n",
      "       _sim_5: beta3\n",
      "       _sim_6: se3\n",
      "       _sim_7: lower1\n",
      "       _sim_8: lower2\n",
      "       _sim_9: lower3\n",
      "      _sim_10: upper1\n",
      "      _sim_11: upper2\n",
      "      _sim_12: upper3\n",
      "\n",
      "Simulations (1,000): .........10.........20.........30.........40.........50...\n",
      "> ......60.........70.........80.........90.........100.........110.........120\n",
      "> .........130.........140.........150.........160.........170.........180.....\n",
      "> ....190.........200.........210.........220.........230.........240.........2\n",
      "> 50.........260.........270.........280.........290.........300.........310...\n",
      "> ......320.........330.........340.........350.........360.........370........\n",
      "> .380.........390.........400.........410.........420.........430.........440.\n",
      "> ........450.........460.........470.........480.........490.........500......\n",
      "> ...510.........520.........530.........540.........550.........560.........57\n",
      "> 0.........580.........590.........600.........610.........620.........630....\n",
      "> .....640.........650.........660.........670.........680.........690.........\n",
      "> 700.........710.........720.........730.........740.........750.........760..\n",
      "> .......770.........780.........790.........800.........810.........820.......\n",
      "> ..830.........840.........850.........860.........870.........880.........890\n",
      "> .........900.........910.........920.........930.........940.........950.....\n",
      "> ....960.........970.........980.........990.........1,000 done\n",
      "\n",
      ". \n",
      ". ci mean _sim_1 _sim_2 _sim_3 _sim_4 _sim_5 _sim_6 _sim_7 _sim_8 _sim_9 _sim_1\n",
      "> 0 _sim_11 _sim_12\n",
      "\n",
      "    Variable |        Obs        Mean    Std. err.       [95% conf. interval]\n",
      "-------------+---------------------------------------------------------------\n",
      "      _sim_1 |      1,000     33.7555    .2044347        33.35433    34.15667\n",
      "      _sim_2 |      1,000    6.350052    .0063805        6.337532    6.362573\n",
      "      _sim_3 |      1,000    25.40088    .0483687        25.30596     25.4958\n",
      "      _sim_4 |      1,000    2.784375    .0036669        2.777179    2.791571\n",
      "      _sim_5 |      1,000    56.29951     .492394        55.33327    57.26576\n",
      "-------------+---------------------------------------------------------------\n",
      "      _sim_6 |      1,000    15.24848    .0152454        15.21857     15.2784\n",
      "      _sim_7 |      1,000     21.3094    .2044141        20.90827    21.71053\n",
      "      _sim_8 |      1,000    19.94351    .0486917        19.84796    20.03905\n",
      "      _sim_9 |      1,000    26.41249    .4935305        25.44401    27.38096\n",
      "     _sim_10 |      1,000     46.2016    .2052189        45.79889    46.60431\n",
      "-------------+---------------------------------------------------------------\n",
      "     _sim_11 |      1,000    30.85825    .0491069        30.76189    30.95462\n",
      "     _sim_12 |      1,000    86.18654     .493069        85.21897    87.15411\n",
      "\n",
      ". \n",
      ". rename _sim_1 beta1e\n",
      "\n",
      ". rename _sim_2 se1e\n",
      "\n",
      ". rename _sim_3 beta2e \n",
      "\n",
      ". rename _sim_4 se2e\n",
      "\n",
      ". rename _sim_5 beta3e \n",
      "\n",
      ". rename _sim_6 se3e\n",
      "\n",
      ". rename _sim_7 ll1e \n",
      "\n",
      ". rename _sim_8 ll2e \n",
      "\n",
      ". rename _sim_9 ll3e \n",
      "\n",
      ". rename _sim_10 ul1e \n",
      "\n",
      ". rename _sim_11 ul2e \n",
      "\n",
      ". rename _sim_12 ul3e \n",
      "\n",
      ". \n",
      ". save missingmean, replace\n",
      "file missingmean.dta saved\n",
      "\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "%%stata\n",
    "\n",
    "simulate beta1 se1 beta2 se2 beta3 se3 lower1 lower2 lower3 upper1 upper2 upper3, reps(1000) seed(1234): missingmean \n",
    "\n",
    "ci mean _sim_1 _sim_2 _sim_3 _sim_4 _sim_5 _sim_6 _sim_7 _sim_8 _sim_9 _sim_10 _sim_11 _sim_12\n",
    "\n",
    "rename _sim_1 beta1e\n",
    "rename _sim_2 se1e\n",
    "rename _sim_3 beta2e \n",
    "rename _sim_4 se2e\n",
    "rename _sim_5 beta3e \n",
    "rename _sim_6 se3e\n",
    "rename _sim_7 ll1e \n",
    "rename _sim_8 ll2e \n",
    "rename _sim_9 ll3e \n",
    "rename _sim_10 ul1e \n",
    "rename _sim_11 ul2e \n",
    "rename _sim_12 ul3e \n",
    "\n",
    "save missingmean, replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "16347553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:28:08.270941Z",
     "start_time": "2025-04-05T10:27:32.600860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". \n",
      ". simulate beta1 se1 beta2 se2 beta3 se3 lower1 lower2 lower3 upper1 upper2 upp\n",
      "> er3, reps(1000) seed(1234): semmlmv \n",
      "\n",
      "      Command: semmlmv\n",
      "       _sim_1: beta1\n",
      "       _sim_2: se1\n",
      "       _sim_3: beta2\n",
      "       _sim_4: se2\n",
      "       _sim_5: beta3\n",
      "       _sim_6: se3\n",
      "       _sim_7: lower1\n",
      "       _sim_8: lower2\n",
      "       _sim_9: lower3\n",
      "      _sim_10: upper1\n",
      "      _sim_11: upper2\n",
      "      _sim_12: upper3\n",
      "\n",
      "Simulations (1,000): .........10.........20.........30.........40.........50...\n",
      "> ......60.........70.........80.........90.........100.........110.........120\n",
      "> .........130.........140.........150.........160.........170.........180.....\n",
      "> ....190.........200.........210.........220.........230.........240.........2\n",
      "> 50.........260.........270.........280.........290.........300.........310...\n",
      "> ......320.........330.........340.........350.........360.........370........\n",
      "> .380.........390.........400.........410.........420.........430.........440.\n",
      "> ........450.........460.........470.........480.........490.........500......\n",
      "> ...510.........520.........530.........540.........550.........560.........57\n",
      "> 0.........580.........590.........600.........610.........620.........630....\n",
      "> .....640.........650.........660.........670.........680.........690.........\n",
      "> 700.........710.........720.........730.........740.........750.........760..\n",
      "> .......770.........780.........790.........800.........810.........820.......\n",
      "> ..830.........840.........850.........860.........870.........880.........890\n",
      "> .........900.........910.........920.........930.........940.........950.....\n",
      "> ....960.........970.........980.........990.........1,000 done\n",
      "\n",
      ". \n",
      ". ci mean _sim_1 _sim_2 _sim_3 _sim_4 _sim_5 _sim_6 _sim_7 _sim_8 _sim_9 _sim_1\n",
      "> 0 _sim_11 _sim_12\n",
      "\n",
      "    Variable |        Obs        Mean    Std. err.       [95% conf. interval]\n",
      "-------------+---------------------------------------------------------------\n",
      "      _sim_1 |      1,000    29.92177    .1667615        29.59453    30.24902\n",
      "      _sim_2 |      1,000    5.142073    .0075898        5.127179    5.156967\n",
      "      _sim_3 |      1,000    40.02574    .0409192        39.94544    40.10603\n",
      "      _sim_4 |      1,000     1.28147    .0018418        1.277856    1.285085\n",
      "      _sim_5 |      1,000    49.54709    .3890354        48.78367    50.31051\n",
      "-------------+---------------------------------------------------------------\n",
      "      _sim_6 |      1,000    12.27739    .0176815        12.24269    12.31209\n",
      "      _sim_7 |      1,000    19.84331    .1651678         19.5192    20.16743\n",
      "      _sim_8 |      1,000    37.51405    .0386898        37.43813    37.58998\n",
      "      _sim_9 |      1,000    25.48341    .3866822        24.72461    26.24221\n",
      "     _sim_10 |      1,000    40.00024    .1696497        39.66733    40.33315\n",
      "-------------+---------------------------------------------------------------\n",
      "     _sim_11 |      1,000    42.53742     .043335        42.45238    42.62246\n",
      "     _sim_12 |      1,000    73.61078    .3944311        72.83677    74.38479\n",
      "\n",
      ". \n",
      ". rename _sim_1 beta1f\n",
      "\n",
      ". rename _sim_2 se1f\n",
      "\n",
      ". rename _sim_3 beta2f \n",
      "\n",
      ". rename _sim_4 se2f\n",
      "\n",
      ". rename _sim_5 beta3f \n",
      "\n",
      ". rename _sim_6 se3f\n",
      "\n",
      ". rename _sim_7 ll1f \n",
      "\n",
      ". rename _sim_8 ll2f \n",
      "\n",
      ". rename _sim_9 ll3f \n",
      "\n",
      ". rename _sim_10 ul1f \n",
      "\n",
      ". rename _sim_11 ul2f \n",
      "\n",
      ". rename _sim_12 ul3f \n",
      "\n",
      ". \n",
      ". save semmlmv, replace\n",
      "file semmlmv.dta saved\n",
      "\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "%%stata\n",
    "\n",
    "simulate beta1 se1 beta2 se2 beta3 se3 lower1 lower2 lower3 upper1 upper2 upper3, reps(1000) seed(1234): semmlmv \n",
    "\n",
    "ci mean _sim_1 _sim_2 _sim_3 _sim_4 _sim_5 _sim_6 _sim_7 _sim_8 _sim_9 _sim_10 _sim_11 _sim_12\n",
    "\n",
    "rename _sim_1 beta1f\n",
    "rename _sim_2 se1f\n",
    "rename _sim_3 beta2f \n",
    "rename _sim_4 se2f\n",
    "rename _sim_5 beta3f \n",
    "rename _sim_6 se3f\n",
    "rename _sim_7 ll1f \n",
    "rename _sim_8 ll2f \n",
    "rename _sim_9 ll3f \n",
    "rename _sim_10 ul1f \n",
    "rename _sim_11 ul2f \n",
    "rename _sim_12 ul3f \n",
    "\n",
    "save semmlmv, replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "20a556f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:38:37.005518Z",
     "start_time": "2025-04-05T10:28:40.290926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". \n",
      ". simulate beta1 se1 beta2 se2 beta3 se3 lower1 lower2 lower3 upper1 upper2 upp\n",
      "> er3, reps(1000) seed(1234): minoaux1 \n",
      "\n",
      "      Command: minoaux1\n",
      "       _sim_1: beta1\n",
      "       _sim_2: se1\n",
      "       _sim_3: beta2\n",
      "       _sim_4: se2\n",
      "       _sim_5: beta3\n",
      "       _sim_6: se3\n",
      "       _sim_7: lower1\n",
      "       _sim_8: lower2\n",
      "       _sim_9: lower3\n",
      "      _sim_10: upper1\n",
      "      _sim_11: upper2\n",
      "      _sim_12: upper3\n",
      "\n",
      "Simulations (1,000): .........10.........20.........30.........40.........50...\n",
      "> ......60.........70.........80.........90.........100.........110.........120\n",
      "> .........130.........140.........150.........160.........170.........180.....\n",
      "> ....190.........200.........210.........220.........230.........240.........2\n",
      "> 50.........260.........270.........280.........290.........300.........310...\n",
      "> ......320.........330.........340.........350.........360.........370........\n",
      "> .380.........390.........400.........410.........420.........430.........440.\n",
      "> ........450.........460.........470.........480.........490.........500......\n",
      "> ...510.........520.........530.........540.........550.........560.........57\n",
      "> 0.........580.........590.........600.........610.........620.........630....\n",
      "> .....640.........650.........660.........670.........680.........690.........\n",
      "> 700.........710.........720.........730.........740.........750.........760..\n",
      "> .......770.........780.........790.........800.........810.........820.......\n",
      "> ..830.........840.........850.........860.........870.........880.........890\n",
      "> .........900.........910.........920.........930.........940.........950.....\n",
      "> ....960.........970.........980.........990.........1,000 done\n",
      "\n",
      ". \n",
      ". ci mean _sim_1 _sim_2 _sim_3 _sim_4 _sim_5 _sim_6 _sim_7 _sim_8 _sim_9 _sim_1\n",
      "> 0 _sim_11 _sim_12\n",
      "\n",
      "    Variable |        Obs        Mean    Std. err.       [95% conf. interval]\n",
      "-------------+---------------------------------------------------------------\n",
      "      _sim_1 |      1,000    29.54683    .0064244        29.53422    29.55943\n",
      "      _sim_2 |      1,000    5.572664    .0013789        5.569958     5.57537\n",
      "      _sim_3 |      1,000    41.44201    .0013083        41.43944    41.44458\n",
      "      _sim_4 |      1,000    1.301768    .0002573        1.301264    1.302273\n",
      "      _sim_5 |      1,000    71.25831    .0383802          71.183    71.33363\n",
      "-------------+---------------------------------------------------------------\n",
      "      _sim_6 |      1,000    10.61355    .0027411        10.60817    10.61893\n",
      "      _sim_7 |      1,000    18.62441    .0087418        18.60725    18.64156\n",
      "      _sim_8 |      1,000    38.89054    .0011773        38.88823    38.89285\n",
      "      _sim_9 |      1,000    50.45575     .040122        50.37702    50.53448\n",
      "     _sim_10 |      1,000    40.46925    .0045538        40.46031    40.47819\n",
      "-------------+---------------------------------------------------------------\n",
      "     _sim_11 |      1,000    43.99348    .0015955        43.99034    43.99661\n",
      "     _sim_12 |      1,000    92.06088    .0373368        91.98762    92.13415\n",
      "\n",
      ". \n",
      ". rename _sim_1 beta1g\n",
      "\n",
      ". rename _sim_2 se1g\n",
      "\n",
      ". rename _sim_3 beta2g \n",
      "\n",
      ". rename _sim_4 se2g\n",
      "\n",
      ". rename _sim_5 beta3g \n",
      "\n",
      ". rename _sim_6 se3g\n",
      "\n",
      ". rename _sim_7 ll1g \n",
      "\n",
      ". rename _sim_8 ll2g \n",
      "\n",
      ". rename _sim_9 ll3g \n",
      "\n",
      ". rename _sim_10 ul1g \n",
      "\n",
      ". rename _sim_11 ul2g \n",
      "\n",
      ". rename _sim_12 ul3g \n",
      "\n",
      ". \n",
      ". save minoaux1, replace\n",
      "file minoaux1.dta saved\n",
      "\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "%%stata\n",
    "\n",
    "simulate beta1 se1 beta2 se2 beta3 se3 lower1 lower2 lower3 upper1 upper2 upper3, reps(1000) seed(1234): minoaux1 \n",
    "\n",
    "ci mean _sim_1 _sim_2 _sim_3 _sim_4 _sim_5 _sim_6 _sim_7 _sim_8 _sim_9 _sim_10 _sim_11 _sim_12\n",
    "\n",
    "rename _sim_1 beta1g\n",
    "rename _sim_2 se1g\n",
    "rename _sim_3 beta2g \n",
    "rename _sim_4 se2g\n",
    "rename _sim_5 beta3g \n",
    "rename _sim_6 se3g\n",
    "rename _sim_7 ll1g \n",
    "rename _sim_8 ll2g \n",
    "rename _sim_9 ll3g \n",
    "rename _sim_10 ul1g \n",
    "rename _sim_11 ul2g \n",
    "rename _sim_12 ul3g \n",
    "\n",
    "save minoaux1, replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "32ba28e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T11:01:01.298816Z",
     "start_time": "2025-04-05T10:38:37.010642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". \n",
      ". simulate beta1 se1 beta2 se2 beta3 se3 lower1 lower2 lower3 upper1 upper2 upp\n",
      "> er3, reps(1000) seed(1234): miaux1 \n",
      "\n",
      "      Command: miaux1\n",
      "       _sim_1: beta1\n",
      "       _sim_2: se1\n",
      "       _sim_3: beta2\n",
      "       _sim_4: se2\n",
      "       _sim_5: beta3\n",
      "       _sim_6: se3\n",
      "       _sim_7: lower1\n",
      "       _sim_8: lower2\n",
      "       _sim_9: lower3\n",
      "      _sim_10: upper1\n",
      "      _sim_11: upper2\n",
      "      _sim_12: upper3\n",
      "\n",
      "Simulations (1,000): .........10.........20.........30.........40.........50...\n",
      "> ......60.........70.........80.........90.........100.........110.........120\n",
      "> .........130.........140.........150.........160.........170.........180.....\n",
      "> ....190.........200.........210.........220.........230.........240.........2\n",
      "> 50.........260.........270.........280.........290.........300.........310...\n",
      "> ......320.........330.........340.........350.........360.........370........\n",
      "> .380.........390.........400.........410.........420.........430.........440.\n",
      "> ........450.........460.........470.........480.........490.........500......\n",
      "> ...510.........520.........530.........540.........550.........560.........57\n",
      "> 0.........580.........590.........600.........610.........620.........630....\n",
      "> .....640.........650.........660.........670.........680.........690.........\n",
      "> 700.........710.........720.........730.........740.........750.........760..\n",
      "> .......770.........780.........790.........800.........810.........820.......\n",
      "> ..830.........840.........850.........860.........870.........880.........890\n",
      "> .........900.........910.........920.........930.........940.........950.....\n",
      "> ....960.........970.........980.........990.........1,000 done\n",
      "\n",
      ". \n",
      ". ci mean _sim_1 _sim_2 _sim_3 _sim_4 _sim_5 _sim_6 _sim_7 _sim_8 _sim_9 _sim_1\n",
      "> 0 _sim_11 _sim_12\n",
      "\n",
      "    Variable |        Obs        Mean    Std. err.       [95% conf. interval]\n",
      "-------------+---------------------------------------------------------------\n",
      "      _sim_1 |      1,000    31.36001    .1790685        31.00862     31.7114\n",
      "      _sim_2 |      1,000    5.026488    .0139061          4.9992    5.053777\n",
      "      _sim_3 |      1,000    41.50636    .0164093        41.47416    41.53856\n",
      "      _sim_4 |      1,000     1.33789    .0059742        1.326167    1.349614\n",
      "      _sim_5 |      1,000    44.76972    .3280355          44.126    45.41344\n",
      "-------------+---------------------------------------------------------------\n",
      "      _sim_6 |      1,000    12.20551    .0160644        12.17399    12.23704\n",
      "      _sim_7 |      1,000    21.50809    .1519217        21.20997    21.80622\n",
      "      _sim_8 |      1,000    38.88409    .0207744        38.84333    38.92486\n",
      "      _sim_9 |      1,000    20.84691    .3139812        20.23077    21.46305\n",
      "     _sim_10 |      1,000    41.21193     .206244        40.80721    41.61665\n",
      "-------------+---------------------------------------------------------------\n",
      "     _sim_11 |      1,000    44.12863    .0195236        44.09031    44.16694\n",
      "     _sim_12 |      1,000    68.69253    .3444025        68.01669    69.36837\n",
      "\n",
      ". \n",
      ". rename _sim_1 beta1h\n",
      "\n",
      ". rename _sim_2 se1h\n",
      "\n",
      ". rename _sim_3 beta2h \n",
      "\n",
      ". rename _sim_4 se2h\n",
      "\n",
      ". rename _sim_5 beta3h \n",
      "\n",
      ". rename _sim_6 se3h\n",
      "\n",
      ". rename _sim_7 ll1h \n",
      "\n",
      ". rename _sim_8 ll2h \n",
      "\n",
      ". rename _sim_9 ll3h \n",
      "\n",
      ". rename _sim_10 ul1h \n",
      "\n",
      ". rename _sim_11 ul2h \n",
      "\n",
      ". rename _sim_12 ul3h \n",
      "\n",
      ". \n",
      ". save miaux1, replace\n",
      "file miaux1.dta saved\n",
      "\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "%%stata\n",
    "\n",
    "simulate beta1 se1 beta2 se2 beta3 se3 lower1 lower2 lower3 upper1 upper2 upper3, reps(1000) seed(1234): miaux1 \n",
    "\n",
    "ci mean _sim_1 _sim_2 _sim_3 _sim_4 _sim_5 _sim_6 _sim_7 _sim_8 _sim_9 _sim_10 _sim_11 _sim_12\n",
    "\n",
    "rename _sim_1 beta1h\n",
    "rename _sim_2 se1h\n",
    "rename _sim_3 beta2h \n",
    "rename _sim_4 se2h\n",
    "rename _sim_5 beta3h \n",
    "rename _sim_6 se3h\n",
    "rename _sim_7 ll1h \n",
    "rename _sim_8 ll2h \n",
    "rename _sim_9 ll3h \n",
    "rename _sim_10 ul1h \n",
    "rename _sim_11 ul2h \n",
    "rename _sim_12 ul3h \n",
    "\n",
    "save miaux1, replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6edbd7e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T16:20:14.497195Z",
     "start_time": "2025-04-05T11:01:01.301350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". \n",
      ". simulate beta1 se1 beta2 se2 beta3 se3 lower1 lower2 lower3 upper1 upper2 upp\n",
      "> er3, reps(1000) seed(1234): miaux100 \n",
      "\n",
      "      Command: miaux100\n",
      "       _sim_1: beta1\n",
      "       _sim_2: se1\n",
      "       _sim_3: beta2\n",
      "       _sim_4: se2\n",
      "       _sim_5: beta3\n",
      "       _sim_6: se3\n",
      "       _sim_7: lower1\n",
      "       _sim_8: lower2\n",
      "       _sim_9: lower3\n",
      "      _sim_10: upper1\n",
      "      _sim_11: upper2\n",
      "      _sim_12: upper3\n",
      "\n",
      "Simulations (1,000): .........10.........20.........30.........40.........50...\n",
      "> ......60.........70.........80.........90.........100.........110.........120\n",
      "> .........130.........140.........150.........160.........170.........180.....\n",
      "> ....190.........200.........210.........220.........230.........240.........2\n",
      "> 50.........260.........270.........280.........290.........300.........310...\n",
      "> ......320.........330.........340.........350.........360.........370........\n",
      "> .380.........390.........400.........410.........420.........430.........440.\n",
      "> ........450.........460.........470.........480.........490.........500......\n",
      "> ...510.........520.........530.........540.........550.........560.........57\n",
      "> 0.........580.........590.........600.........610.........620.........630....\n",
      "> .....640.........650.........660.........670.........680.........690.........\n",
      "> 700.........710.........720.........730.........740.........750.........760..\n",
      "> .......770.........780.........790.........800.........810.........820.......\n",
      "> ..830.........840.........850.........860.........870.........880.........890\n",
      "> .........900.........910.........920.........930.........940.........950.....\n",
      "> ....960.........970.........980.........990.........1,000 done\n",
      "\n",
      ". \n",
      ". ci mean _sim_1 _sim_2 _sim_3 _sim_4 _sim_5 _sim_6 _sim_7 _sim_8 _sim_9 _sim_1\n",
      "> 0 _sim_11 _sim_12\n",
      "\n",
      "    Variable |        Obs        Mean    Std. err.       [95% conf. interval]\n",
      "-------------+---------------------------------------------------------------\n",
      "      _sim_1 |      1,000    24.95661    .0229473        24.91158    25.00165\n",
      "      _sim_2 |      1,000    4.800662    .0014553        4.797806    4.803518\n",
      "      _sim_3 |      1,000    38.61344    .0067181        38.60026    38.62662\n",
      "      _sim_4 |      1,000    1.232736    .0002915        1.232164    1.233308\n",
      "      _sim_5 |      1,000    38.67783     .044842        38.58984    38.76583\n",
      "-------------+---------------------------------------------------------------\n",
      "      _sim_6 |      1,000    11.48743     .003155        11.48124    11.49362\n",
      "      _sim_7 |      1,000    15.54732    .0205344        15.50702    15.58761\n",
      "      _sim_8 |      1,000    36.19728    .0062849        36.18494    36.20961\n",
      "      _sim_9 |      1,000    16.16247     .042354        16.07936    16.24558\n",
      "     _sim_10 |      1,000    34.36591    .0254513        34.31597    34.41586\n",
      "-------------+---------------------------------------------------------------\n",
      "     _sim_11 |      1,000     41.0296    .0071707        41.01553    41.04368\n",
      "     _sim_12 |      1,000    61.19319    .0480024          61.099    61.28739\n",
      "\n",
      ". \n",
      ". rename _sim_1 beta1i\n",
      "\n",
      ". rename _sim_2 se1i\n",
      "\n",
      ". rename _sim_3 beta2i \n",
      "\n",
      ". rename _sim_4 se2i\n",
      "\n",
      ". rename _sim_5 beta3i \n",
      "\n",
      ". rename _sim_6 se3i\n",
      "\n",
      ". rename _sim_7 ll1i \n",
      "\n",
      ". rename _sim_8 ll2i \n",
      "\n",
      ". rename _sim_9 ll3i \n",
      "\n",
      ". rename _sim_10 ul1i \n",
      "\n",
      ". rename _sim_11 ul2i \n",
      "\n",
      ". rename _sim_12 ul3i \n",
      "\n",
      ". \n",
      ". save miaux100, replace\n",
      "file miaux100.dta saved\n",
      "\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "%%stata\n",
    "\n",
    "simulate beta1 se1 beta2 se2 beta3 se3 lower1 lower2 lower3 upper1 upper2 upper3, reps(1000) seed(1234): miaux100 \n",
    "\n",
    "ci mean _sim_1 _sim_2 _sim_3 _sim_4 _sim_5 _sim_6 _sim_7 _sim_8 _sim_9 _sim_10 _sim_11 _sim_12\n",
    "\n",
    "rename _sim_1 beta1i\n",
    "rename _sim_2 se1i\n",
    "rename _sim_3 beta2i \n",
    "rename _sim_4 se2i\n",
    "rename _sim_5 beta3i \n",
    "rename _sim_6 se3i\n",
    "rename _sim_7 ll1i \n",
    "rename _sim_8 ll2i \n",
    "rename _sim_9 ll3i \n",
    "rename _sim_10 ul1i \n",
    "rename _sim_11 ul2i \n",
    "rename _sim_12 ul3i \n",
    "\n",
    "save miaux100, replace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03978464",
   "metadata": {},
   "source": [
    "# Reference List\n",
    "\n",
    "Allison, P. (2012a) ‘Handling Missing Data by Maximum Likelihood’, SAS Global Forum [Preprint].\n",
    "\n",
    "Allison, P. (2012b) ‘Why Maximum Likelihood is Better Than Multiple Imputation’, Statistical Horizons, 9 July. Available at: https://statisticalhorizons.com/ml-better-than-mi/ (Accessed: 15 May 2023).\n",
    "\n",
    "Allison, P. (2015) ‘Maximum Likelihood is Better than Multiple Imputation: Part II’, Statistical Horizons, 5 May. Available at: https://statisticalhorizons.com/ml-is-better-than-mi/ (Accessed: 15 May 2023).\n",
    "\n",
    "Carpenter, J.R. and Kenward, M. (2012) Multiple imputation and its application. John Wiley & Sons.\n",
    "\n",
    "Collins, L.M., Schafer, J.L. and Kam, C.-M. (2001) ‘A comparison of inclusive and restrictive strategies in modern missing data procedures.’, Psychological Methods, 6(4), pp. 330–351. Available at: https://doi.org/10.1037/1082-989X.6.4.330.\n",
    "\n",
    "Enders, C.K. (2001) ‘A Primer on Maximum Likelihood Algorithms Available for Use With Missing Data’, Structural Equation Modeling: A Multidisciplinary Journal, 8(1), pp. 128–141. Available at: https://doi.org/10.1207/S15328007SEM0801_7.\n",
    "\n",
    "Enders, C.K. (2010) Applied missing data analysis. New York: Guilford Press (Methodology in the social sciences).\n",
    "\n",
    "Hardt, J. et al. (2013) ‘Multiple Imputation of Missing Data: A Simulation Study on a Binary Response’, Open Journal of Statistics, 03(05), pp. 370–378. Available at: https://doi.org/10.4236/ojs.2013.35043.\n",
    "\n",
    "Hawkes, D. and Plewis, I. (2006) ‘Modelling non-response in the National Child Development Study’, Journal of the Royal Statistical Society, 169(3), pp. 479–491. Available at: https://doi.org/10.1111/j.1467-985X.2006.00401.x.\n",
    "\n",
    "Hyuk Lee, J. and Huber Jr., J.C. (2021) ‘Evaluation of Multiple Imputation with Large Proportions of Missing Data: How Much Is Too Much?’, Iranian Journal of Public Health [Preprint]. Available at: https://doi.org/10.18502/ijph.v50i7.6626.\n",
    "\n",
    "Johnson, D.R. and Young, R. (2011) ‘Toward Best Practices in Analyzing Datasets with Missing Data: Comparisons and Recommendations’, Journal of Marriage and Family, 73(5), pp. 926–945. Available at: https://doi.org/10.1111/j.1741-3737.2011.00861.x.\n",
    "\n",
    "Jones, M.P. (1996) ‘Indicator and Stratification Methods for Missing Explanatory Variables in Multiple Linear Regression’, Journal of the American Statistical Association, 91(433).\n",
    "\n",
    "Kang, H. (2013) ‘The prevention and handling of the missing data’, Korean Journal of Anesthesiology, 64(5), p. 402. Available at: https://doi.org/10.4097/kjae.2013.64.5.402.\n",
    "\n",
    "Little, R.J., Carpenter, J.R. and Lee, K.J. (2022) ‘A Comparison of Three Popular Methods for Handling Missing Data: Complete-Case Analysis, Inverse Probability Weighting, and Multiple Imputation’, Sociological Methods & Research, p. 004912412211138. Available at: https://doi.org/10.1177/00491241221113873.\n",
    "\n",
    "Little, R.J. and Rubin, D.B. (2019) Statistical analysis with missing data. John Wiley & Sons.\n",
    "\n",
    "Lynch, J. and Von Hippel, P.T. (2013) ‘Efficiency Gains from Using Auxiliary Variables in Imputation’, Cornell University Library [Preprint].\n",
    "\n",
    "Madley-Dowd, P. et al. (2019) ‘The proportion of missing data should not be used to guide decisions on multiple imputation’, Journal of Clinical Epidemiology, 110, pp. 63–73. Available at: https://doi.org/10.1016/j.jclinepi.2019.02.016.\n",
    "\n",
    "Marsden, P.V. and Wright, J.D. (eds) (2010) Handbook of survey research. Second edition. Bingley: Emerald Group Publ.\n",
    "\n",
    "Seaman, S.R. et al. (2012) ‘Combining Multiple Imputation and Inverse‐Probability Weighting’, Biometrics, 68(1), pp. 129–137. Available at: https://doi.org/10.1111/j.1541-0420.2011.01666.x.\n",
    "\n",
    "Seaman, S.R. and White, I.R. (2013) ‘Review of inverse probability weighting for dealing with missing data’, Statistical Methods in Medical Research, 22(3), pp. 278–295. Available at: https://doi.org/10.1177/0962280210395740.\n",
    "\n",
    "Silverwood, R. et al. (2021) ‘Handling missing data in the National Child Development Study: User guide (Version 2).’\n",
    "\n",
    "Von Hippel, P.T. (2009) ‘How to Impute Interactions, Squares, and Other Transformed Variables’, Sociological Methodology, 39(1), pp. 265–291. Available at: https://doi.org/10.1111/j.1467-9531.2009.01215.x.\n",
    "\n",
    "White, I.R., Royston, P. and Wood, A.M. (2011) ‘Multiple imputation using chained equations: Issues and guidance for practice’, Statistics in Medicine, 30(4), pp. 377–399. Available at: https://doi.org/10.1002/sim.4067.\n",
    "\n",
    "Young, R. and Johnson, D.R. (2011) ‘Imputing the Missing Y’s: Implications for Survey Producers and Survey Users’, Proceedings of the AAPOR Conference Abstracts [Preprint]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
