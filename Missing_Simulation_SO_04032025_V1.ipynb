{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec54d881",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T14:40:20.719188Z",
     "start_time": "2025-03-04T14:40:20.716796Z"
    }
   },
   "source": [
    "# A Simulation Study Comparing Handling Missing Data Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5d7d00",
   "metadata": {},
   "source": [
    "Scott Oatley (s2265605@ed.ac.uk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff165975",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Missing data is a threat to the accurate reporting of substantive results within data analysis. While handling missing data strategies are widely available, many studies fail to account for missingness in their analysis. Those who do engage in handling missing data analysis sometimes engage in less than-gold-standard approaches that leave their substantive interpretation open toward methodological and statistical critique. These gold standard measures; multiple imputation (MI) and full information maximum likelihood (FIML), are rarely compared with one another. Mostly due to discipline-based preference, there has been little work on assessing if using one of these methods or the other would produce less biased estimates. This paper seeks to establish the real impact of implementing handling missing data methods upon statistical analysis and secondly, provide a direct comparison between multiple imputation and full information maximum likelihood methods of handling missing data. A simulation is performed on a variety of handling missing data strategies using a set seed for reproducibility. This simulation of methods is repeated 1000 times, and the 95% confidence intervals of the betas and standard errors of each model are presented to demonstrate the standard each handling missing data method provides the researcher. The results of this analysis confirm that under a missing at-random assumption, methods such as listwise deletion are not at all appropriate in handling missing data. Furthermore, naive methods are analysed, such as single-use imputation and found to be equally unattractive. Finally, results show that multiple imputations appear to provide the most accurate reporting of original ‘non-missing’ models compared to full information maximum likelihood techniques, though the difference is not large enough to rule it out as a viable ‘gold standard’. A discussion of statistical and time-based efficiency is also provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ac98d9",
   "metadata": {},
   "source": [
    "### Key Words\n",
    "\n",
    "MCAR, MAR, MNAR, Multiple Imputation, FIML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ea1b3",
   "metadata": {},
   "source": [
    "## Justification For Notebook\n",
    "\n",
    "Providing a literate workflow of any scientific investigations should be a baseline requiste of empirical research. Publishing a Jupyter Notebook allows individuals to not only reproduce the empirical work held within, it also provides a robust justification for each step in the research process. This process promotes open-science practices and encourages others to do the same. \n",
    "\n",
    "Encouraging others to openly look at ones work invites critique. These critiques are welcome. The hope of using Notebooks is to have a much more organic and engaging research process whereby the research does not simply end once a paper is published. Critical comments can and will be incorporated into this Notebook to further research practices. \n",
    "\n",
    "By providing a literate workflow where research, theory, justification, and 'footnote' analysis are all recorded in one place. This notebook invites a widespread auideance, ranging from other academics, to interested stakeholders. Whilst the language used in this Notebook is one intended for an academic auidence, the workflow presented should be possible for anyone that reads the Notebook and takes a methodical step-by-step approach to its application. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcd2f19",
   "metadata": {},
   "source": [
    "### Using Stata\n",
    "\n",
    "As Jupyter is a language agnostic program, the use of language used for analysis is left up to the individual researcher. For this Notebook, Stata is employed for all statistical analysis. Stata is a proprietary software and researchers MUST have access to Stata in order to undertake data analyses within the Jupyter notebook. A primary goal of extending this analysis is to undertake it in a different non-properiatory software. \n",
    "\n",
    "Using Stata requires enabling the use of the 'stata kernel' in Jupyter. The instructions for which have been outlined in meticulous details in Connelly and Gayle (2019). For the sake of promoting a consistent introduction base for Jupyter Notebooks, and in an attempt to avoid needless confusing rhetoric at the beginning of such Notebooks, their original instructions are pasted below. I thank them for their work in pioneering best practices in the use of Notebooks for social scientific analysis: \n",
    "\n",
    "#### Using Stata via Magic Cells\n",
    "\n",
    "The approach for this Notebook uses Stata via magic cells. This facility can be downloaded and installed from [this github repository](https://github.com/TiesdeKok/ipystata).\n",
    "\n",
    "At the command prompt you need to type:\n",
    "\n",
    "pip install ipystata\n",
    "\n",
    "In a code cell before using Stata you must type:\n",
    "\n",
    "import ipystata\n",
    "\n",
    "and then run the cell.\n",
    "\n",
    "Each cell will now be a Stata code cell as long as you start your syntax with:\n",
    "\n",
    "%%stata\n",
    "\n",
    "For example to get a summary of the variables in Stata the cell should include the following code:\n",
    "\n",
    "%%stata\n",
    "summarize\n",
    "\n",
    "further information on using Stata via magic [is available here](https://dev-ii-seminar.readthedocs.io/en/latest/notebooks/Stata_in_jupyter.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8c885f",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "- [Background](#Background)\n",
    "- [Missing Data Theory](#Missing)\n",
    "- Simulated Data\n",
    "- Preparation Programs\n",
    "- Descriptive Statistics \n",
    "- Missingness Models\n",
    "- Discussion of Results\n",
    "- Conclusions\n",
    "- Notes\n",
    "- Supplementary Materials\n",
    "- References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e3994",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Missing Data pervades social scientific research. Without appropriately accounting for missing data in reserach agendas, this missingness can have stark consequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfc5f37",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Missing data is a term that describes an instance whereby either an item (variable) or unit (person/entire observation) is missing from the sample. Missing data is common within social science research and even more ubiquitous within a longitudinal context where wave attrition and non-response are a rule rather than exception. The primary concern surrounding missing data is that dependent upon the type of missingness, there is potential to affect inferences made by the analysis of longitudinal studies (Hawkes and Plewis, 2006: 479; Silverwood et al., 2021). The various factors that account for sample attrition in the datasets presented thus far have the potential to present real issues as they relate to comprehensive data analysis. \n",
    "\n",
    "Missingness attributed to death or emigration are considered ‘natural’ from the original sample. Those, however, that either refuse continued survey participation or complete surveys partially may present problems of biased estimates if the missingness is not appropriately discussed. Put simply, if there is an identified pattern within the missing data present within a given analysis this has the potential to influence results and alter substantive conclusions had this missingness not occurred – or been appropriately accounted for. \n",
    "\n",
    "When a unit (case/observation) provides information on every possible item (variable) within a survey, this is a complete record. When information on certain items within a survey are missing, this is then called an incomplete record. When only part of all possible items is missing from the unit this is called item non-response. When all possible items are missing this is called unit non-response. Unit non-response is a natural consequence of a longitudinal survey design. The statistical techniques discussed henceforth will focus on item non-response because it is far more common in practice than the alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cdfb1f",
   "metadata": {},
   "source": [
    "# Missing Data Theory\n",
    "\n",
    "The investigation of missingness within data, through the creation of tables and graphical summaries of the full data compared to partially observed variables can beginning to identify the mechanisms underlying the patterns of missingness. The terminology that defines these mechanisms is unfortunately rather obtuse and confusing. The follow paragraphs attempt to best explain and define the key missingness mechanisms that are used with the methodological literature surrounding missing data whilst attempting to provide a clear and concise understanding. \n",
    "\n",
    "The first major missingness mechanism is called Missing Completely At Random (MCAR).  Suppose that only one variable Y has missing data, and that another set of variables represented by the vector X, is always observed (Marsden and Wright, 2010). The data is MCAR if the probability that Y is missing does not depend on either X or Y itself. Evaluating the assumption that missingness on Y depends on some observed variable in X is straightforward. Allison (2012) uses the example of income depending on gender by testing whether the proportions of men and women who report their income differ – a logistic regression in which the dependent variable is the response indicator could be estimated, and significant coefficients would suggest a violation of the MCAR mechanism (ibid). Testing whether missingness on Y does not depend on Y itself is much more complicated. Unless we have existing linked data such as tax records in the income example, it is almost impossible to evaluate this assumption. The upside of an MCAR mechanism is that estimated coefficients will not provide biased results in the presence of data Missing Completely At Random, however their estimation may be less precise (Kang, 2013). \n",
    "\n",
    "The second missingness mechanism is missing at random (MAR). Data on Y is considered MAR if the probability that Y is missing does not depend on Y, once we control for X. MAR allows for missingness on Y to depend on other variables so long as it does not depend on Y itself. \n",
    "\n",
    "Finally, missing not at random (MNAR) means missingness depends on unobserved values (Silverwood et al. 2021), and that the probability that Y is missing depends on Y itself, after adjusting for X (Marsden and Wright, 2010). For example, people who have been arrested may be less likely to report their arrest status. \n",
    "\n",
    "If data is found to be MAR or MCAR, then approaches like multiple imputation (MI), Full Information Maximum likelihood (FIML), and inverse probability weighting (IPW) are made available – the former being extensively documented with the NCDS (Hawkes and Plewis 2006). These ‘gold standard’ approaches to handling missing data have also been found to produce optimal estimates in the MNAR case but it is difficult to have confidence that any given MNAR model is correct (Marsden and Wright, 2010)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d339a1",
   "metadata": {},
   "source": [
    "# Methods to Handle Missing Data\n",
    "\n",
    "The following section explores each major attempt at handling missing data. These techniques and methods are bundled into ‘standards’ to express the level of appropriate application most of these approaches fall into. While the majority are placed in ‘less than gold standard’ approaches, that does not necessarily make them devoid of utility. There are three main categories that handling missing data techniques fall into: ‘gold standard’, ‘less than gold standard’, and ‘inadequate standard’. Each method for each category will be explained. \n",
    "\n",
    "## Inadequate Standard\n",
    "\n",
    "The first inadequate standard given a MAR mechanism is present is listwise deletion. Listwise deletion removes all observations from the data with a missing value in one or more of the variables included in the analysis. This is also known as Complete Records Analysis (CRA). The CRA approach is unpredictable; there is no way to know the consequences of this loss of information if data is found to be MAR (Carpenter and Kenward, 2012). When data is found to be MAR, a CRA approach is inadequate at handling missing data.\n",
    "\n",
    "Depending on the variable (either metric or categorical) a simple approach to handling missing data would be to use a single mean or single modal imputation. This, in the example of a categorical variable, takes the mode of the value in said variable and imputes that modal value across all missing values in the data. Single imputation ignores all uncertainty and always underestimates the variance in each model. Advocates of this approach argue that whilst not perfect this approach doesn’t delete a single case and incorporates all available information into a given model. However, this method does not have any confidence in its results. There is a possibility that the estimates from this method may fall close to the true range; of course, the exact opposite is equally likely. The use of single-use imputation has been consistently and conclusively shown to perform poorly except under exceptionally special conditions (Collins, Schafer and Kam, 2001; Little and Rubin, 2019). For these reasons, single use imputation is an inadequate method to handle missing data. \n",
    "\n",
    "## Less than Gold Standard \n",
    "\n",
    "Dummy variable adjustment is another method of handling missing data that falls into the ‘less than gold standard’ category. Dummy variable adjustment may appear to be in the same category of handling missing data methods as single use imputation. This is, however, not the case. Dummy variable adjustment is where all missingness at the given variable is coded to a value within the model. In the example of a binary dummy variable, all missingness is coded to either equal zero or equal one. This does have the identical appeal to the single-use imputation of deleting no cases and incorporates all information into the regression model. However, there is a substantive difference between the two techniques. For the simple model of data missing at Y variable, a dummy variable adjustment will not provide the ‘true’ estimates but if the complete records analysis is compared to a model where all missingness equals zero and another model where all missingness equals one, then the range of the estimates can be located. Whilst Jones (1996) demonstrated that dummy variable adjustment yields biased parameter estimates even when the data is MCAR, the ability to provide a range of the estimates does provide some utility to this technique. Given a MAR example where the reported estimates are a reduced form from their ‘true’ values, iff the complete case analysis and both dummy variable adjustment models present a beta coefficient that is throughout all models positive, one can present those results similar to how we ought to interpret log odds. The results would present evidence for a positive coefficient – though the exact size is unknown, some information can be gathered. For this reason, dummy variable adjustment provides some utility in certain missing data scenarios. This technique has the most utility in scenarios where missingness is so great that it begins to stretch the abilities of even gold-standard techniques. This method for handling missing data is not perfect, but it does provide utility and allows the use of data that has large amounts of missingness. \n",
    "\n",
    "Another method that deals with missing data is the use of survey weights. Survey weights consider missingness. Inverse Probability Weighting (IPW) creates weighted copies of complete records to remove selection bias introduced by missing data. Whilst IPW is a method of dealing with missing data, alternatives such as multiple imputation are regarded as much more efficient as IPW only determines weights from incomplete cases and partially observed cases are discarded in the weighted analysis. Due to this, weighted estimates can have unacceptably high variance (Seaman et al., 2012; Seaman and White, 2013; Little, Carpenter and Lee, 2022).As such IPWs are placed in the ‘less than gold standard’ category of handling missing data approaches.  \n",
    "\n",
    "## Gold Standard\n",
    "\n",
    "There are two ‘gold standard’ approaches to handling missing data, Multiple Imputation (MI) and Maximum Likelihood (ML). Referring to the latter method first, there are currently three ML estimation algorithms for use when missing data is present with either an MCAR or MAR mechanism. The first is the multiple-group method, whereby a sample is divided into subgroups which each share the same pattern of missing data. A likelihood function is computed for each of the subgroups and the groupwise likelihood functions are accumulated across the entire sample and maximised. There are some practical issues of implementing this multiple-group based ML approach (Enders, 2001). The major drawback of this approach however is that it is a group level, rather than individual level ML estimation. Another ML estimation is the expectation-maximisation (EM). This estimation uses a two-step iterative procedure where missing observations are filled in or imputed and the unknown parameters are estimated using maximum likelihood missing data algorithms. The EM approach can only be used to obtain ML estimates of a mean vector and covariance matrix and as a result standard errors will be negatively biased and bootstrapping is recommended (Enders, 2001). The final ML approach discussed here is the Full Information Maximum Likelihood (FIML) estimation. It has also been called the raw maximum likelihood estimation for its likelihood function being calculated at the individual. It is also exceptionally easy to implement compared to the other estimation procedures discussed (Enders, 2001). For these reasons, going forward ML discussions of handling missing data specifically refer to the FIML approach rather than the multiple-group or EM approach. \n",
    "\n",
    "Multiple Imputation is the second of the ‘Gold standard’ handling missing data methods. Multiple imputation generates replacement values or imputations for the missing data values and repeats this procedure over many iterations to produce a ‘semi-Bayesian’ framework for the most appropriate fit of estimates. For multiple imputation models to be compared to a complete records analysis, the former needs to be ‘‘congenial’’ (White, Royston and Wood, 2011) with the latter. Congeniality or consistency in this respect means that the same variables in the complete record analysis are identical to those included in multiple imputation. Suppose the variables between complete records analysis and multiple imputation models differ. In that case, the correct variance/covariance matrix will not be estimated, and a substantive comparison between the two will become impossible and impracticable due to a loss of statistical power (Von Hippel, 2009; Lynch and Von Hippel, 2013). \n",
    "\n",
    "Multivariate imputation by chained equations (MICE) is a form of multiple imputation that fills in or imputes missing data within a given dataset through iterative predictive models or k imputations. This specification is required when imputing a variable that must only take on specific values, such as the categorical nature of the economic activity response variable within the current analytical model. Using MICE, each imputation k is drawn from the posterior distribution of the parameters in the given imputation model, and then the model itself is imputed (Carpenter and Kenward, 2012). To create the kth imputation, new parameters are drawn from the posterior distribution. Multiple Imputation following MICE draws from Bayesian influences on the distribution of missing data upon observed data. An essential advantage of Multiple Imputation is that it can be applied for data missing at the response variable or its covariates (Carpenter and Kenward, 2012).\n",
    "\n",
    "Multiple imputation uses auxiliary variables – variables not included in the main model but are used when setting the data to be imputed. The auxiliary variables' main function is to improve the predictive ability of the imputation model over and above the information recovered from just using the information provided by the analytical variables in the model (Collins, Schafer and Kam, 2001). Auxiliary variables are essential when there are high levels of missingness upon a given variable (Johnson and Young, 2011; Young and Johnson, 2011). There is no strict threshold for what an auxiliary variable needs to be included within the imputation; however, some have recommended an r > 0.4 on at least one of the analytical variables within the model (Allison, 2012a). However, this is disputed (Enders, 2010). Others, such as Silverwood et al. (2021), argue that if an auxiliary variable is predictive of the outcome variable, it makes them suitable for inclusion within the imputation model. An auxiliary variable does not have the requirement that the given variable has to have complete information to be valuable – auxiliary variables can still be influential when they have missingness (Enders, 2010). \n",
    "\n",
    "Multiple Imputation can be implemented easily and readily across software platforms, unlike FIML. Multiple imputation does, however, have some drawbacks. It can be a lengthy procedure that has the potential to induce human error due to the need to select auxiliary variables, set the correct data for imputation, and set the correct seed etc. There is also a time efficiency argument, whereby for multiple imputation, if the dataset is large, or there are large amounts of missingness, then the time to impute the model of interest can take a large amount of time. MI is an attractive method because it is practical and widely applicable (Carpenter and Kenward, 2012). \n",
    "\n",
    "Whilst original literature on missing data and MI typically referred to large datasets with marginal levels of missingness, contemporary studies and simulations have increasingly stretched and stress-tested the limits of MI (Hardt et al., 2013). A simulation by Hardt et al (2013) demonstrated that large amounts of missingness can be present within a model without breaking down MI or FIML mechanisms (ibid). Whilst their simulation stops at n=200 where 40 per cent missingness is acceptable, there general argument is that the greater the n the larger the missingness can be within a model without breaking MI or FIML so long as the models themselves are appropriately specified. Imputation-based models are consistently found to outperform a CRA in both absolute bias and Root Mean Squared Error (RMSE) with increasing levels of missingness (Hyuk Lee and Huber Jr., 2021). The most extreme case from Madely-Down et al (2019) demonstrates that so long as the imputation model is properly specified and data are MAR then unbiased results can be obtained even with up to 90 per cent missingness. An imputation model compared to a CRA can achieve a reduction in 99.97 per cent bias when missingness is at 90 per cent (ibid).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5450ab67",
   "metadata": {},
   "source": [
    "# Comparisons of Gold Standard Methods\n",
    "\n",
    "Paul Allison, in a series of articles (Allison, 2012a, 2012b, 2015), argues that FIML is 1) more straightforward to implement, 2) FIML has no incompatibility between an imputation model and an analysis model, 3) FIML produces a deterministic result rather than a different result every time, and 4) FIML is asymptomatically efficient. \n",
    "\n",
    "Firstly, MI does have greater variability than FIML, but that increased choice in model selection is not necessarily a negative so long as proper procedures are followed. In fact, greater variability of choice has the potential to make MI a more attractive candidate for dealing with missingness over FIML. Secondly, MI models only run into an incompatibility problem when the MI model is inconsistent with the CRA model – something that, with appropriate testing and open science practices detailing the model construction, should not happen. Thirdly, MI models are deterministic, provided the same seed is used each time you run the imputation. The only time this would not be plausible would be when open science practices were not followed, and fellow researchers could not access the MI seed. Finally, the argument that FIML is asymptotically efficient only holds to a certain extent. MI models reach asymptotic efficiency by running an infinite number of imputations – though you can reach near full efficiency with a relatively small number of imputations, Allison (2015) argues, around 10. Overall, whilst FIML does offer some advantages, there is nothing so considerable theoretically as to desire FIML over MI on the condition that they both perform at near identical rates. So long as open science procedures are upheld, most major critiques of MI are dealt with. \n",
    "\n",
    "There are very few comparisons between FIML and MI approaches to missing data, making it hard to assess whether one method is more efficient at dealing with missing data than the other. Before conducting any missing data methods on the NCDS data, a simulation is performed to assess the strengths of a range of handling missing data approaches, with the intent to directly compare FIML and MI methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c107f7a",
   "metadata": {},
   "source": [
    "# Methods\n",
    "\n",
    "Both FIML and MI practices require data to either be MCAR or MAR. A FIML approach can be achieved in Stata by using the ‘sem’ command – using structural equation modelling and using the ‘mlvm’ estimation option (mlvm means FIML). MI can also be achieved in Stata using the ‘mi’ commands using a semi-Bayesian approach that includes auxiliary variables. There are also other handling missing data methods available such as: single mean imputation and coding all data=0 OR =1. These practices are typically considered ‘bad’ ways of handling missing data but are included in the simulation as a comparison to FIML and MI methods. \n",
    "\n",
    "The full simulation  takes the form of 1000 iterations of a random normal distribution of 1000 observations around a normally distributed metric dependent variable and three independent dummy variables that share an identical distribution. Each independent variable has the same level of correlation associated with the dependent variable. This is to allow for a point of comparison when MAR missingness is injected into one variable and not the others to see what happens when handling missing data practices are implemented. Each model is isolated in its own program whereby a simulation is called using the programs function with an identical seed set to all models. The 95 per cent confidence intervals of the mean betas and standard errors for all variables within each model are gathered and reported. \n",
    "\n",
    "This dependent variable and three independent variables form a basic OLS linear regression model that is called the ‘Complete Records God Model’. Named as such because no model in a normal social scientific framework would have all observations not missing and have prior knowledge of what the ‘complete’ model would have looked like if their model did have some element of missingness. In addition to this ‘God’ model the same regression is computed using the structural equation modelling framework in Stata to confirm the results would be identical. The next model is where missingness is introduced. Missingness is injected into independent variable three. This missingness accounts for 49 per cent missingness in the model. This amount of missingness is right on the cusp of what contemporary literature on multiple imputation and FIML allow. Dummy variable adjustment is produced whereby all missingness is coded as =0 and another is produced =1. Next a single use modal imputation is used – the same framework as a single use mean imputation but because the variable is categorical mode is used over mean. Finally, an FIML model under the SEM framework is produced alongside three different forms of Multiple Imputation models. The first is an MI with 10 imputations and no auxiliary variables, the second is an MI with 10 imputations and auxiliary variables, and finally the last model is an MI with 100 imputations and auxiliary variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec913d43",
   "metadata": {},
   "source": [
    "# Preparation Programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c18b2a0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T14:39:28.345456Z",
     "start_time": "2025-03-04T14:39:27.133223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ___  ____  ____  ____  ____ ®\n",
      " /__    /   ____/   /   ____/      18.0\n",
      "___/   /   /___/   /   /___/       SE—Standard Edition\n",
      "\n",
      " Statistics and Data Science       Copyright 1985-2023 StataCorp LLC\n",
      "                                   StataCorp\n",
      "                                   4905 Lakeway Drive\n",
      "                                   College Station, Texas 77845 USA\n",
      "                                   800-STATA-PC        https://www.stata.com\n",
      "                                   979-696-4600        stata@stata.com\n",
      "\n",
      "Stata license: Unlimited-user network, expiring 14 Sep 2025\n",
      "Serial number: 401809305318\n",
      "  Licensed to: Scott Oatley\n",
      "               University of Edinburgh\n",
      "\n",
      "Notes:\n",
      "      1. Unicode is supported; see help unicode_advice.\n",
      "      2. Maximum number of variables is set to 5,000 but can be increased;\n",
      "          see help set_maxvar.\n"
     ]
    }
   ],
   "source": [
    "import stata_setup\n",
    "\n",
    "stata_setup.config(\"C:\\Program Files\\Stata18\", \"se\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9061eea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T15:33:34.062846Z",
     "start_time": "2025-03-04T15:33:34.024525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". \n",
      ". capture program drop regressbase\n",
      "\n",
      ". \n",
      ". program regressbase, rclass\n",
      "  1. drop _all\n",
      "  2. \n",
      ". *Set number of observations \n",
      ". set obs 1000 \n",
      "  3. \n",
      ". * Generate metric independent variable #1\n",
      ". drawnorm x1, n(1000) means(50) sds(15)\n",
      "  4. \n",
      ". * Generate metric independent variable #2\n",
      ". drawnorm x2, n(1000) means(10) sds(20)\n",
      "  5. \n",
      ". * Generate metric independent variable #3\n",
      ". drawnorm x3, n(1000) means(5) sds(2)\n",
      "  6. \n",
      ". * Generate metric dependent variable\n",
      ". gen y = 1*x1 + 5*x2 + 3*x3 + rnormal()\n",
      "  7. \n",
      ". * Perform OLS linear regression model\n",
      ". regress y x1 x2 x3\n",
      "  8. \n",
      ". * Store the betas and standard errors for each independent variable in model\n",
      ". gen beta1 = _b[x1]\n",
      "  9. gen se1 = _se[x1]\n",
      " 10. \n",
      ". gen beta2 = _b[x2]\n",
      " 11. gen se2 = _se[x2]\n",
      " 12. \n",
      ". gen beta3 = _b[x3]\n",
      " 13. gen se3 = _se[x3]\n",
      " 14. \n",
      ". * Store the Root Mean Square Error (RMSE) for this model\n",
      ". gen error1 = e(rmse)\n",
      " 15. \n",
      ". * End Program\n",
      ". end\n",
      "\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "%%stata\n",
    "\n",
    "capture program drop regressbase\n",
    "\n",
    "program regressbase, rclass\n",
    "drop _all\n",
    "\n",
    "*Set number of observations \n",
    "set obs 1000 \n",
    "\n",
    "* Generate metric independent variable #1\n",
    "drawnorm x1, n(1000) means(50) sds(15)\n",
    "\n",
    "* Generate metric independent variable #2\n",
    "drawnorm x2, n(1000) means(10) sds(20)\n",
    "\n",
    "* Generate metric independent variable #3\n",
    "drawnorm x3, n(1000) means(5) sds(2)\n",
    "\n",
    "* Generate metric dependent variable\n",
    "gen y = 1*x1 + 5*x2 + 3*x3 + rnormal()\n",
    "\n",
    "* Perform OLS linear regression model\n",
    "regress y x1 x2 x3\n",
    "\n",
    "* Store the betas and standard errors for each independent variable in model\n",
    "gen beta1 = _b[x1]\n",
    "gen se1 = _se[x1]\n",
    "\n",
    "gen beta2 = _b[x2]\n",
    "gen se2 = _se[x2]\n",
    "\n",
    "gen beta3 = _b[x3]\n",
    "gen se3 = _se[x3]\n",
    "\n",
    "* Store the Root Mean Square Error (RMSE) for this model\n",
    "gen error1 = e(rmse)\n",
    "\n",
    "* End Program\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806bfda4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
